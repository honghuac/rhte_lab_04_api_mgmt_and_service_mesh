:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= API Mgmt and Service Mesh Lab

.Goals
* Inject Istio Envoy proxy configs into an Apicast gateway
* Configure an Istio Egress Route for an Apicast gateway
* End-to-end distributed tracing of a MSA application using Jaeger implementation of the _OpenTracing_ specification

.Prerequisite
* Skills
** Completion of the _APIs as a Business_ lab
** Completion of the _MSA and Service Mesh_ lab
* Tools
** `curl` utility
** `sed` utility
** `istioctl` utility
** `oc` version 3.9 utility

:numbered:

== Overview

An _Istio_ based service mesh introduces features that somewhat resumble features found in an API management solution.

The purpose of this lab is to apply both a _service mesh_ and an API management solution to a container native application.

Along the way, we'll compare and contrast some of the features between API Mgmt and Service Mesh using Red Hat products and community projects.


=== Configuration of CNAP solutions

The emphasis of this lab is on configuration: specifically, configuration of a _Cloud Native App Platform (CNAP)_ that includes _API Management_ and a _Service Mesh_.

Although this topic and the technologies employed in this lab should be of high interest to a developer responsible for business services running in production in a Cloud Native App Platform,
Students of this lab will not write any business logic.
Development of CNAP business and integration logic can be written in a wide variety of development platforms offered by Red Hat:

. Red Hat Openshift Application Runtimes (RHOAR)
. Red Hat Fuse on OpenShift

Details about these Red Hat development platforms are out of scope for this lab however please enroll in Red Hat GPTE's courses on these topics if you are interested.

=== Background

 TO-DO

. What is Red Hat's API Management solution ?  Red Hat 3scale
. Describe Red Hat's investments in the _Istio_ project ?
. What is the scope of API Management ?
. What is the scope of a Service Mesh ?


=== Alternatives

==== OpenTracing APIcast vs Istio
 
Of related interest is that the upstream community APIcast gateway from Red Hat's 3scale product already bundles _OpenTracing_ libraries.
This allows it already to particpate in distributed tracing when paired with implementations such as zepplin or Jaeger.
 
_Observability_ is a major focus of a _service mesh_ and Istio in particular.
As such, Istio supports distributed tracing out of the box by bundling Jaeger.
 
In this lab, you will not use the _OpenTracing_ libraries bundled in APIcast.
Instead, APIcast will be injected with the _Envoy_ sidecar of Istio and the _Envoy_ sidecar will handle tracing communication of the _OpenTracing_ mixer of Istio.
 
==== 3scale mixer adapter
 
TO-DO :  Juaquim will elaborate on this on Aug 9 during the 3scale F2F .
 
 


== Lab Asset Overview

=== Environment Variables

Before getting started, you'll want to open a terminal window and set the following environment variables that will be used throughout the duration of this lab.

ifdef::showscript[]
If student lab environment and 3scale tenants were provisioned using the ocp-workload-rhte-mw-api-mesh ansible role, then student details can be found in:

/tmp/3scale_tenants/user_info_file.txt

endif::showscript[]

-----
######  Instructor will provide the values to these environment variables #######

$ export REGION=<provided by your instructor>
$ export GUID=<provided by your instructor>
$ export OCP_PASSWD=<provided by your instructor>
$ export API_ADMIN_ACCESS_TOKEN=<provided by your instructor>



#######  Using above variables, the following can be copied and pasted in same terminal window   ########

$ export OCP_WILDCARD_DOMAIN=apps.$REGION.openshift.opentlc.com
$ export TENANT_NAME=$OCP_USERNAME-3scale

# Notice the value of this variable is being set to the route to the 3scale _system-provider_ 
$ export THREESCALE_PORTAL_ENDPOINT=https://${API_ADMIN_ACCESS_TOKEN}@$TENANT_NAME-admin.$OCP_WILDCARD_DOMAIN

$ export OCP_USERNAME=user$GUID
$ export OCP_PROJECT=rhte-mw-api-mesh-$GUID
-----

ifdef::showscript[]

# Potential alternative using service endpoint (may need to use master)
$ export THREESCALE_PORTAL_ENDPOINT=http://${API_ADMIN_ACCESS_TOKEN}@system-provider.3scale-mt-adm0.svc.cluster.local

endif::showscript[]

=== 3scale AMP access

Your lab environment includes access to a multi-tenant 3scale AMP installation.

For the purpose of this lab, you will serve as the administrator of your own 3scale _tenant_ (aka: _domain_)

. Log into the admin portal of your 3scale AMP environment using the information to do provided by your instructor

. To access the admin portal of your 3scale environment, point to your browser to the output of the following:
+
-----
$ echo -en "\n\nhttps://$TENANT_NAME-admin.$OCP_WILDCARD_DOMAIN\n\n"
-----

. Authenticate using the values of $OCP_USERNAME and $OCP_PASSWD   (Your 3scale credentials are the same as your OCP credentials).
+
image::images/3scale_login.png[]


=== OpenShift access

You lab environent is built on Red Hat's OpenShift Container Platform.

Access to your OCP resources can be gained via both the `oc` utility as well as the OCP web console.

. Log into OpenShift
+
-----
$ oc login https://master.$REGION.openshift.opentlc.com -u $OCP_USERNAME -p $OCP_PASSWD
-----

. Ensure that your `oc` client is the same minor release version as the server:
+
-----
$ oc version

oc v3.9.30
kubernetes v1.9.1+a0ce1bc657
features: Basic-Auth GSSAPI Kerberos SPNEGO

Server https://master.a4ec.openshift.opentlc.com:443
openshift v3.9.31
kubernetes v1.9.1+a0ce1bc657
-----

.. In the above example, notice that version of the `oc` client is of the same minor release (v3.9.30) of the OpenShift server (v3.9.31)
.. There a known subtle problems with using a version of the `oc` client that is different from your target OpenShift server.

. View existing projects:
+
-----
$ oc get projects

... 

istio-system                                      Active
rhte-mw-api-mesh-user1   rhte-mw-api-mesh-user1   Active
-----

.. Your OCP user has been provided with _view_ and _edit_ access to the central _istio-system_ namespace with all _control plane_ Istio functionality.
+
Later in this lab, you'll use a utility called _istioctl_ .
This utility will need both view and edit privlidges to the _istio-system_ namespace.

.. The namespace _rhte-mw-api-mesh-*_ is where you will be working throughout the duration of this lab.

. Switch to your  OpenShift project
+
-----
$ oc project $OCP_PROJECT
-----

. Log into OpenShift Web Console
.. Many OpenShift related tasks found in this lab can be completed in the Web Console (as an alternative to using the `oc` utility`.
.. To access, point to your browser to the output of the following:
+
-----
$ echo -en "\n\nhttps://master.$REGION.openshift.opentlc.com\n\n"
-----

.. Authenticate using the values of $OCP_USERNAME and $OCP_PASSWD


=== Deployment vs DeploymentConfig 

Your lab assets consist of a mix of OpenShift _Deployment_ and _DeploymentConfig_ resources.

The _Deployment_ construct is the more recent Kubernetes equivalent of what has always been in OpenShift:  _DeploymentConfig_.

The _istioctl_ utility (introduced later in this lab) of Istio requires the use of the Kubernetes _Deployment_ resource.
Subsequently, for the purpose of this lab, we'll use the Kubernetes _Deployment_ type (instead of DeploymentConfig) for all microservices.

The CoolStore catalog service included in your lab environment connects to a MongoDB database.
This MongoDB database is managed by Kubernetes using an OpenShift DeploymentConfig instead of a Kubernetes Deployment.
The reason for this is the the OpenShift DeploymentConfig provides more features than a Deployment to include the use of _lifecycle_ hooks.
This is important because the MongoDB in your lab is pre-seeded with data using a _post deployment_ life-cycle hook.
This _post deployment_ lifecycle hook is simply ignored when attempting to use a Kubernetes Deployment.


If you interested in learning more about the differences between Kubernetes _Deployments_ and OCP _DeploymentConfigurations_, please see
link:https://docs.openshift.com/container-platform/3.10/dev_guide/deployments/kubernetes_deployments.html#kubernetes-deployments-vs-deployment-configurations[this documentation].

=== CoolStore Catalog Service


==== OpenShift objects

. Review DeploymentConfig
+
-----
$ oc get dc -n $OCP_PROJECT

...

NAME              REVISION   DESIRED   CURRENT   TRIGGERED BY
catalog-mongodb   1          1         1         config,image(mongodb:3.4)
-----

. Review Deployment
+
-----
$ oc get deploy -n $OCP_PROJECT

...

NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
catalog-service   1         1         1            1           4m
-----

. Review running pods -n $OCP_PROJECT
+
-----
$ oc get pods

...

NAME                          READY     STATUS      RESTARTS   AGE
catalog-mongodb-1-clsz4       1/1       Running     0          11m
catalog-service-1-dqb28       1/1       Running     0          11m

...
-----

. Retrieve the URL of the unsecured _catalog_ route:
+
[source,text]
----
$ export NAKED_CATALOG_ROUTE=http://$(oc get route catalog-unsecured -o template --template='{{.spec.host}}' -n $OCP_PROJECT)
----

. Via the catalog route, retrieve the pre-seeded data in the Mongo database:
+
-----
$ curl -X GET "$NAKED_CATALOG_ROUTE/products"
-----
+
.Sample Output
-----
...

{
  "itemId" : "444435",
  "name" : "Oculus Rift",
  "desc" : "The world of gaming has also undergone some very unique and compelling tech advances in recent years. Virtual reality, the concept of complete immersion into a digital universe through a special headset, has been the white whale of gaming and digital technology ever since Nintendo marketed its Virtual Boy gaming system in 1995.",
  "price" : 106.0
}
-----

==== Invoke _Open API Specification_ docs

The link:https://swagger.io/docs/specification/about/[OpenAPI Specification^] (formerly "Swagger Specification") is an API description format for REST APIs. link:https://swagger.io/[Swagger^] is a set of open-source tools built around the OpenAPI specification that can help you design, build, document, and consume REST APIs.

Swagger documentation is available for the REST endpoints of the catalog microservice.

. Display the URL for your project:
+
[source,text]
----
$ echo $NAKED_CATALOG_ROUTE
----

. Copy and paste the URL into a web browser.
* Expect to see the Swagger docs for the REST endpoints:
+
image::images/swagger-ui-coolstore-catalog.png[]

. Click *GET /products Get a list of products* to expand the item.
. Click the *Try it out* button, then click *Execute*.
. View the REST call response:
+
image::images/swagger-ui-response.png[]

. Use the Swagger UI to test the other REST endpoints for the catalog microservice.


== Apicast: Plain

In this lab, you will provision your own 3scale _apicast_ gateway to manage your CoolStore _catalog_ service.

image::images/deployment_apicast.png[]


This apicast gateway is similar to an unmodified supported APIcast gateway with the exception that management of your APIcast gw occurs via a Kubernetes _deployment_ as opposed to an OCP _deploymentconfig_. 

Your APIcast gateway will pull configurations from the pre-provisioned multi-tenant 3scale AMP.

In a later section of this lab, this same _apicast_ gateway will be Istio enabled.

=== Deploy Apicast

. Retrieve Apicast template
+
-----
$ curl -o $HOME/lab/3scale-apicast-2.2.yml \
          https://raw.githubusercontent.com/gpe-mw-training/3scale_onpremise_implementation_labs/master/resources/rhte/3scale-apicast-2.2.yml
-----

. Review Apicast template
+
-----
$ cat $HOME/lab/3scale-apicast-2.2.yml | more
-----

. Check your knowledge
+
TO-DO

. Create Apicast staging related resources in OpenShift:
+
-----
$ oc new-app \
     -f $HOME/lab/3scale-apicast-2.2.yml \
     --param THREESCALE_PORTAL_ENDPOINT=$THREESCALE_PORTAL_ENDPOINT \
     --param APP_NAME=catalog-stage-apicast \
     --param ROUTE_NAME=catalog-stage-apicast-$OCP_USERNAME \
     --param WILDCARD_DOMAIN=$OCP_WILDCARD_DOMAIN \
     --param THREESCALE_DEPLOYMENT_ENV=sandbox \
     --param APICAST_CONFIGURATION_LOADER=lazy \
     -n $OCP_PROJECT > $HOME/lab/catalog-stage-apicast_details.txt
-----

. Create Apicast production related resources in OpenShift:
+
-----
$ oc new-app \
     -f $HOME/lab/3scale-apicast-2.2.yml \
     --param THREESCALE_PORTAL_ENDPOINT=$THREESCALE_PORTAL_ENDPOINT \
     --param APP_NAME=catalog-prod-apicast \
     --param ROUTE_NAME=catalog-prod-apicast-$OCP_USERNAME \
     --param WILDCARD_DOMAIN=$OCP_WILDCARD_DOMAIN \
     --param THREESCALE_DEPLOYMENT_ENV=production \
     --param APICAST_CONFIGURATION_LOADER=lazy \
     -n $OCP_PROJECT > $HOME/lab/catalog-prod-apicast_details.txt
-----

. Resume the intially paused deploy objects:
+
-----
$ oc rollout resume deploy catalog-stage-apicast catalog-prod-apicast -n $OCP_PROJECT
-----

=== Configure and Test API Mgmt

In this section of the lab, you can optionally smoke test the management of your _catalog_ RESTful services using your 3scale AMP and APIcast gateways.

Guidance is provided for both experienced and inexperienced 3scale users.
Choose only one.

==== Configure & Test: Experienced 3scale users

If you are already proficient with 3scale, then configure and test the management of your _catalog_ RESTful API as per the following :

. Your Apicast gateways started correctly and the value of the _THREESCALE_ENDPOINT_ makes sense.
. Create an API proxy service called _catalog_service_ and configure it to use the APIcast gateway and an API key for security.
. Create an application plan called: _catalog_app_plan_
. Create an application called: _catalog_app_
. Capture the API key for the application and set its value as the following environment variable in your shell terminal:  _CATALOG_USER_KEY_ .
. Configure the _Integration_ section of your _catalog_service_ .
. Test the _/products_ resource of your _catalog_ RESTful via both your staging and production APIcast gateways.

Proceed to the section: <<apicast_istio>>

==== Configure & Test:  Inexperienced 3scale users

If you are new to API management using 3scale, then follow the instructions found in the <<configuretestapi>> section of the appendix of this lab.

Upon completion, return back to this point and proceed with next section: <<apicast_istio>>.

[[apicast_istio]]
== Apicast: Istio enabled

=== Overview

Your lab assets should now consist of a _catalog_ RESTful service as well as both production and staging apicast gateways that are configured to send _authrep_ reports to the central multi-tenant 3scale AMP.

In this section of the lab, you will now create new APIcast gateways that are injected with the Istio _Envoy_ proxy.

The _Envoy_ proxy will interoperate with _service mesh control plane_ functionality found in the _istio-system_ namespace.

TO-DO:  Architecture diagram

=== Procedure

. View special privileges:
+
-----

TO-DO: view privileged scc on default sa

-----


. Retrieve yaml representation of current apicast production deployment:
+
-----
$ oc get deploy catalog-prod-apicast -n $OCP_PROJECT -o yaml > $HOME/lab/catalog-prod-apicast.yaml
-----

. Differentiate your Istio enabled apicast gateway from your existing APIcast gateway:
+
-----
$ sed -i "s/catalog-prod-apicast/$OCP_USERNAME-catalog-prod-apicast-istio/" $HOME/lab/catalog-prod-apicast.yaml
-----
+
TO-DO:  elaborate as to why $OCP_USERNAME is utilized

. Place the deployment in a paused state:
+
-----
$ sed -i "s/replicas:\ 1/replicas: 1\n  paused: true/" $HOME/lab/catalog-prod-apicast.yaml
-----

. View configmap in `istio-system` project
+
-----
$ oc describe configmap istio -n istio-system | more
-----
+
TO-DO:  Provide details and elaboration of the configs in the configmap.

. Inject Istio configs into a new apicast deployment
+
-----

$ istioctl kube-inject \
           -f $HOME/lab/catalog-prod-apicast.yaml \
           > $HOME/lab/catalog-prod-apicast-istio.yaml
-----

. View Istio injected APIcast gateway deployment descriptor:
+
-----
$ cat $HOME/lab/catalog-prod-apicast-istio.yaml | more
-----

. Deploy a new Istio enabled apicast production gateway:
+
-----
$ oc create \
     -f $HOME/lab/catalog-prod-apicast-istio.yaml \
     -n $OCP_PROJECT
-----

. Inject required resource limits and requests into Istio related containers :
+
There is a clusterquota assigned to your OCP user.
This clusterquota requires that all containers specify its _limits_ and _requests_.
+
-----
TO-DO
-----

. Resume the paused deployment:
+
-----
$ oc rollout resume deploy/$OCP_USERNAME-catalog-prod-apicast-istio
-----





== Jaeger UI

TO-DO : Elaborate OpenTracing spans in Jaeger UI

-----
$ echo -en "\n\nhttp://"$(oc get route/tracing -o template --template {{.spec.host}} -n istio-system)"\n\n"
-----

. In the _Find Traces_ panel, scroll down to locate the traces associated with your OCP user name:
+
image::images/trace_dropdown_selection.png[]

. Click `Find Traces` and once you've done so, you should see an overview with timeline of all of your traces:
+
image::images/trace_overview.png[]

== Catalog Service: Istio enabled



=== Procedure

. Retrieve yaml representation of current _catalog service_ deployment:
+
-----
$ oc get deploy catalog-service -n $OCP_PROJECT -o yaml > $HOME/lab/catalog-service.yaml
-----

. Differentiate your Istio enabled catalog service from your existing catalog service:
+
-----
$ sed -i "s/catalog-service/$OCP_USERNAME-catalog-service/" $HOME/lab/catalog-service.yaml
-----

. Place the deployment in a paused state:
+
-----
$ sed -i "s/replicas:\ 1/replicas: 1\n  paused: true/" $HOME/lab/catalog-service.yaml
-----


. Inject Istio configs into a new catalog service deployment
+
-----

$ istioctl kube-inject \
           -f $HOME/lab/catalog-service.yaml \
           > $HOME/lab/catalog-service-istio.yaml
-----

. View Istio injected catalog service deployment descriptor:
+
-----
$ cat $HOME/lab/catalog-service-istio.yaml | more
-----

. Deploy a new Istio enabled apicast production gateway:
+
-----
$ oc create \
     -f $HOME/lab/catalog-service-istio.yaml \
     -n $OCP_PROJECT
-----

. Inject required resource limits and requests into Istio related containers :
+
There is a clusterquota assigned to your OCP user.
This clusterquota requires that all containers specify its _limits_ and _requests_.
+
-----
TO-DO
-----

. Resume the paused deployment:
+
-----
$ oc rollout resume deploy/$OCP_USERNAME-catalog-service
-----

. Modify _service_ to route to new Istio enabled _apicast_
+
-----
$ oc patch service/catalog-service \
   --patch '{"spec":{"selector":{"deployment":"'$OCP_USERNAME'-catalog-service"}}}'
-----

. Make sure that your `$CATALOG_USER_KEY` environment variable is set:
+
-----
$ echo $CATALOG_USER_KEY

d59904ad4515522ecccb8b81c761a283
-----

. From the terminal, execute the following:
+
-----
$ curl -v -k `echo -en "\nhttps://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY\n"`
-----



== 3scale Analytics

Return back to your 3scale AMP as the domain admin and navigate to the _Analytics_ tab at the top.

image::images/3scale_analytics.png[]

Notice that the _hits_ metric for your _catalog_service_ API is automatically depicted.
3scale analytics can depict the total count of _hits_ on both the API as well as the API method level graphed over time.

Your API analytics are currently course grained in that the _hits_ are the sum of invocations on all methods of your catalog service.
Defining of fine grained _methods_ and _mappings_ for your catalog API will subsequently provide for more fine grained analytics at the method level.

The analytics provided by 3scale compliment the distributed tracing capabilities of Jaeger.




== Conclusions

As you know, Openresty is Nginx + luaJIT, and right now, we only get OpenTracing information for the "Nginx" part of it, there aren't any OpenTracing libraries for lua.
We are working on being able to use the OpenTracing C++ libraries from LUA, so we can create spans directly from it, and gain even more visibility into APIcast internals. 
For example, this could help debug if that custom policy you just installed is making things slower.


== Questions

TO-DO :  questions to test student knowledge of the concepts / learning objectives of this lab

== Appendix


[[configuretestapi]]
=== Configure and Test API Mgmt (Optional)

In this section, you define a service that manages access to the Coolstore Catalog service that has already been provisioned for you.

The activities in this section are also found in the pre-req courses but is additionally provided here as a refresher for your conveniance.

==== Define Catalog Service

. From the 3scale AMP Admin Portal home page, navigate to the *API* tab.
. On the far right, click image:images/create_service_icon.png[].
. Enter `catalog_service` for the *Name* and *System Name*.
. Select *NGINX APIcast self-managed* *Gateway* type and not a plugin:
+
image::images/apicast_gw.png[]

. Scroll down the page and for the *Authentication* type, select *API Key (user_key)*:
+
image::images/select_api_key.png[]

. Click *Create Service*.

==== Create Application Plan

Application plans define access policies for your API.

. From the *Overview* page of your new `catalog_service`, scroll to the *Published Application Plans* section.
. Click image:images/create_app_plan_icon.png[]:
+
image::images/create_app_plan.png[]

. Enter `catalog_app_plan` for the *Name* and *System name*:

. Click *Create Application Plan*.

==== Create Application

In this section, you associate an application to an API consumer account.
This generates a _user key_ to the application based on the details previously defined in the application plan.
The user key is used as a query parameter to the HTTP request to invoke your business services via your on-premise APIcast gateway.

. Navigate to the *Developers* tab.
. Select the `Developer` account.
. Create Application
.. Click the *0 Applications* link at the top:
+
NOTE: A default application may have already been created (in which case the link will indicate 1 Application, not 0).
If so, this default application is typically associated with the out-of-the-box `API` service (which is not what you want).
If it exists, feel free to click on default application to identify which service it is associated with and then delete it.

.. Click image:images/create_app_icon.png[].
.. Fill in the *New Application* form as follows:
... *Application plan*: `catalog_app_plan`
... *Service Plan*: `Default`
... *Name*: `catalog_app`
... *Description*: `catalog_app`
+
image::images/create_catalog_app.png[]

.. Click *Create Application*.

. On the details page for your new application (or the default application automatically created), find the API *User Key*:
+
image::images/new_catalog_user_key.png[]

. Create an environment variable set to this user key:
+
-----
$ export CATALOG_USER_KEY=<the catalog app user key>
-----

==== Stage Service Integration

In this section, you define an _API proxy_ to manage your _catalog_ RESTful business service.

. In the 3scale AMP Admin Portal, navigate to the *APIs* tab.
. From your `catalog_service` service, select *Application Plans*.
. For the  `catalog_app_plan` and click the *Publish* link:
+
image::images/publish_app_plan.png[]
. From your `catalog_service` service, select *Integration*.
. Click *Add the base URL of your API and save the configuration*.
+
image::images/add_base_url.png[]
+
* This takes you to a page that allows you to configure the apicast staging and production environments.

. Populate the *Configuration: configure & test immediately in the staging environment* form as follows:
.. *Private Base URL*:
... Enter the internal DNS resolvable URL to your Catalog business service.
... The internal URL will be the output of the following:
+
-----
$ echo -en "\n\nhttp://catalog-service.$OCP_PROJECT.svc.cluster.local:8080\n\n"
-----

.. *Staging Public Base URL*: Populate this field with the output from the following command:
+
-----
$ echo -en "\n`oc get route catalog-stage-apicast-$OCP_USERNAME -n $OCP_PROJECT --template "https://{{.spec.host}}"`:443\n\n"
-----

.. *Production Public Base URL*: Populate this field with the output from the following command:
+
-----
$ echo -en "\n`oc get route catalog-prod-apicast-$OCP_USERNAME -n $OCP_PROJECT --template "https://{{.spec.host}}"`:443\n\n"
-----

.. *API test GET request*: Enter `/products`.

** Expect to see a test cURL command populated with the API key assigned to you for the `catalog_app_plan`:
+
image::images/apikey_shows_up.png[]
+
.. If not, go back through the steps to create an Application Plan and corresponding Application.
+
NOTE: When there are multiple developer accounts, Red Hat 3scale API Management uses the default developer account that is created with every new API provider account to determine which user key to use. When creating new services, the 3scale AMP sets the first application from the first account subscribed to the new service as the default.

. Click *Update & test in Staging Environment*
.. In doing so, the `apicast-stage` pod invokes your backend _catalog_ business service as per the `Private Base URL`.
.. The page should turn green with a message indicating success.
+
image::images/stage_success.png[]

. Click *Back to Integration & Configuration*:
. Click *Promote v. 1 to Production*:
+
image::images/stage_and_prod.png[]

Your 3scale by Red Hat service is configured.
Next, the configuration details of your service need to be propagated to your on-premise APIcast gateway.

==== Refresh APIcast at boot
Every time a configuration change is made to an api proxy or application plan, the production APIcast gateways need to be refreshed with the latest changes.

The APIcast gateways are configured to refresh the latest configuration information from the API management platform every 5 minutes.
When this internal NGINX timer is triggered, you see log statements in your APIcast gateway similar to the following:

.Sample Output
-----
[debug] 36#36: *3574 [lua] configuration_loader.lua:132: updated configuration via timer:

....

[info] 36#36: *3574 [lua] configuration_loader.lua:160: auto updating configuration finished successfuly, context: ngx.timer
-----

For the purpose of this lab, instead of potentially waiting for 5 minutes, you can simply bounce your apicast pods .

. Delete existing apicast related pods+
+
-----
$ for i in `oc get pod | grep "apicast" | awk '{print $1}'`; do oc delete pod $i; done
-----
+
Kubernetes will detect the absence of these pods and start new ones.
+
Because the value of the _APICAST_CONFIGURATION_LOADER_ environment variable in the pod is set to `boot`, the service proxy configuration from the 3scale AMP will automatically be pulled upon restart.

. Tail the log of the new apicast production pod.

* A debug-level log statement similar to the following appears:
+
.Sample Output
-----
[lua] configuration_store.lua:103: configure(): added service 2555417742084 configuration with hosts: catalog-prod-apicast-user1.apps.7777.thinkpadratwater.com, catalog-stage-apicast-user1.apps.7777.thinkpadratwater.com ttl: 300
-----

==== Test Catalog Business Service

In this section, you invoke your Catalog business service via your on-premise APIcast gateway.

. Make sure that your `$CATALOG_USER_KEY` environment variable is still set:
+
-----
$ echo $CATALOG_USER_KEY
-----

. From the terminal, execute the following:
+
-----
$ curl -v -k `echo -en "\nhttps://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY\n"`
-----
+
.Sample Output
-----
...

{
  "itemId" : "444435",
  "name" : "Oculus Rift",
  "desc" : "The world of gaming has also undergone some very unique and compelling tech advances in recent years. Virtual reality, the concept of complete immersion into a digital universe through a special headset, has been the white whale of gaming and digital technology ever since Nintendo marketed its Virtual Boy gaming system in 1995.",
  "price" : 106.0
}
-----

. If you are still tailing the log of your `apicast` pod, expect to see statements similar to this:
+
.Sample Output
-----
...

2018/08/06 19:07:46 [info] 24#24: *19 [lua] backend_client.lua:108: authrep(): backend client uri: http://backend-listener.3scale-mt-adm0:3000/transactions/authrep.xml?service_token=a4e0949f1b677611870dab3fb7c142df50871d1eca3d1c9f1615dd514c937df4&service_id=103&usage%5Bhits%5D=1&user_key=ccc4cbae7a44b363a6cd5907a54ff2f9 ok: true status: 200 body:  while sending to client, client: 172.17.0.1, server: _, request: "GET /products?user_key=ccc4cbae7a44b363a6cd5907a54ff2f9 HTTP/1.1", host: "catalog-service.rhte-mw-api-mesh-user1.svc.cluster.local"

...

-----




ifdef::showscript[]

export API_ADMIN_ACCESS_TOKEN=a3f62f9107104dc3aba1acb906e23d15d5227108e5bd11a3337de5f94f691dec

echo -en "\n\ncurl -k ${THREESCALE_PORTAL_ENDPOINT}/admin/api/services.json\n\n"                                    :   test retrival of proxy service info from system-provider

oc rsh `oc get pod | grep "apicast-istio" | awk '{print $1}'`                                                       :   ssh into istio enabled apicast gw

oc logs -f  `oc get pod | grep "apicast-istio" | awk '{print $1}'` -c $OCP_USERNAME-catalog-prod-apicast-istio      :   log of istio enabled apicast gw

for i in `oc get pod | grep "apicast-istio" | awk '{print $1}'`; do oc delete pod $i; done                          :   Re-dploy Istio enabled Apicast gateway


TO-DO
  1)  Is a liveness probe necessary for apicast ?  Apicast appears to error out on its own during boot problems.
  2)  With liveness and readiness probes removed, apicast boot error behaves differently depending on whether it is injected with istio
        - istio injected :   apicast boot errors cause fail-over the first 2 or 3 times.  Then no longer any errors.
        - no istio       :   apicast continues to fail upon boot errors

      Turns out envoy proxy is blocking outbound calls at boot for about 1 minute or so
      All outbound calls from primary pods (ie:  apicast invocation to THREESCALE_PORTAL_ENDPOINT and vert.x / fabric8 invocation to kubernetes API to query for configmap) during that time are blocked.

      https://github.com/istio/istio/issues/3533        :   startup time of istio-proxy causes comm issues for up to 30 seconds


  3) investigate istio-ingress
        OCP ha-proxy -> istio-ingress -> apicast gw -> catalog service

  4) when apicast is in info log level, why does it stop rebooting itself when a THREESCALE_PORTAL_ENDPOINT related problem is encountered ?
     when apicast is in debug log level, it continues to cycle when it encounters a THREESCALE_PORTAL_ENDPOINT problem .

  5) with istio injected apicast, boot doesn't start however a curl within the same pod on THREESCALE_PORTAL_ENDPOINT does work

  6) allow user write access to istio-system to allow for execution of:  "istioctl create"

endif::showscript[]

ifdef::showscript[]

. Change _APICAST_LOG_LEVEL_ environment variable to _info_:
+
-----
$ oc patch deploy/$OCP_USERNAME-catalog-prod-apicast-istio \
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"'$OCP_USERNAME'-catalog-prod-apicast-istio", "env": [{"name":"APICAST_LOG_LEVEL","value":"info" }]}]}}}}'
-----
+
The _info_ log level in APIcast gateway actually provides more useful connection error details than does the _debug_ log level.
+
This will become important because we are about to encounter a connection related error now that Istio is introduced .
The connection problem will be in the apicast gateway at boot when it attempts to pull (using the value set in its THREESCALE_PORTAL_ENDPOINT env variable) _proxy-config_ information from the _system-provider_ of 3scale AMP.

. Investigate _apicast_ provisioning problem
+
-----
$ oc logs -f `oc get pod | grep "apicast-istio" | awk '{print $1}'` -c $OCP_USERNAME-catalog-prod-apicast-istio

...

2018/08/02 08:32:23 [warn] 23#23: *2 [lua] remote_v2.lua:163: call(): failed to get list of services: invalid status: 0 url: https://user1-3scale-admin.apps.7777.thinkpadratwater.com/admin/api/services.json, context: ngx.timer
2018/08/02 08:32:23 [info] 23#23: *2 [lua] remote_v1.lua:98: call(): configuration request sent: https://user1-3scale-admin.apps.7777.thinkpadratwater.com/admin/api/nginx/spec.json, context: ngx.timer
2018/08/02 08:32:23 [error] 23#23: *2 peer closed connection in SSL handshake, context: ngx.timer
2018/08/02 08:32:23 [warn] 23#23: *2 [lua] remote_v1.lua:108: call(): configuration download error: handshake failed, context: ngx.timer
ERROR: /opt/app-root/src/src/apicast/configuration_loader.lua:57: missing configuration
stack traceback:
	/opt/app-root/src/src/apicast/configuration_loader.lua:57: in function 'boot'
	/opt/app-root/src/libexec/boot.lua:6: in function 'file_gen'
	init_worker_by_lua:49: in function <init_worker_by_lua:47>
	[C]: in function 'xpcall'
	init_worker_by_lua:56: in function <init_worker_by_lua:54>

-----

.. From the log file, notice that initial warning indicates a failure "to get list services" from the 3scale AMP _system-provider_ service.
.. Interestingly, it is possible to ssh into the apicast gateway and execute the curl utility to manually retrieve the list of proxy service configuration:
+
-----
$ oc rsh `oc get pod -n $OCP_PROJECT | grep "apicast-istio" | awk '{print $1}'` \
     curl -k ${THREESCALE_PORTAL_ENDPOINT}/admin/api/services.json \
     | python -m json.tool | more

...

{
    "services": [
        {
            "service": {
                "backend_version": "1",
                "created_at": "2018-08-07T11:13:03Z",
                "end_user_registration_required": true,
                "id": 3,
                "links": [
                    {
                        "href": "https://user1-3scale-admin.apps.7777.thinkpadratwater.com/admin/api/services/3/metrics",
                        "rel": "metrics"
                    },


....
-----
+
Why would you expect that the _curl_ utility to be able to pull the _service-proxy_ data when rsh'd into the apicast gateway but the apicast gateway itself fails to do so ?

=== Apply custom _Egress Route_

In this section, you create a custom Istio _ServiceEntry_ that allows your APIcast gateway to connect to the _backend-listener_ of the multi-tenant 3scale AMP.

. Configure a custom Istio _Egress Route_ for Apicast gateway config file:
+
-----
$ echo \
    "apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: $OCP_USERNAME-catalog-apicast-egress-rule
spec:
  hosts:
  - $TENANT_NAME-admin.$OCP_WILDCARD_DOMAIN
  location: MESH_EXTERNAL
  ports:
  - name: https-443
    number: 443
    protocol: HTTPS 
  resolution: DNS" \
 > $HOME/lab/catalog-apicast-egressrule.yaml
-----

. Inject configs from the configmap in _istio-system_ namespace:
+
-----
$ istioctl create -f $HOME/lab/catalog-apicast-egressrule.yaml -n $OCP_PROJECT
-----

. Modify _service_ to route to new Istio enabled _apicast_
+
-----
$ oc patch service/catalog-prod-apicast \
   --patch '{"spec":{"selector":{"app":"'$OCP_USERNAME'-catalog-prod-apicast-istio"}}}'
-----

. Make sure that your `$CATALOG_USER_KEY` environment variable is set:
+
-----
$ echo $CATALOG_USER_KEY

d59904ad4515522ecccb8b81c761a283
-----

. From the terminal, execute the following:
+
-----
$ curl -v -k `echo -en "\nhttps://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY\n"`
-----
+
.Sample Output
-----
...

{
  "itemId" : "444435",
  "name" : "Oculus Rift",
  "desc" : "The world of gaming has also undergone some very unique and compelling tech advances in recent years. Virtual reality, the concept of complete immersion into a digital universe through a special headset, has been the white whale of gaming and digital technology ever since Nintendo marketed its Virtual Boy gaming system in 1995.",
  "price" : 106.0
}
-----

endif::showscript[]



