:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= API Mgmt and Service Mesh Lab

.Goals
* Inject Istio Envoy proxy configs into an Apicast gateway
* Configure an Istio Egress Route for an Apicast gateway
* End-to-end distributed tracing of a MSA application using Jaeger implementation of the _OpenTracing_ specification

.Prerequisite
* Skills
** Completion of the _APIs as a Business_ lab
** Completion of the _MSA and Service Mesh_ lab
* Tools
** `curl` utility
** `sed` utility
** `istioctl` utility
** `oc` version 3.9 utility

:numbered:

== Overview

_API Management_ facilitates relationships between API consumers and producers.
It lowers the link:https://en.wikipedia.org/wiki/Transaction_cost[transaction costs] (ie: search costs, price discovery, policing and enforcement costs, etc ) that would otherwise hinder mutually beneficial exchange between API consumer and producer.
It is often the case that these API producers and consumers do not personally know each other.

A _service mesh_ provides the technical resilience and _observability_ needed to facilitate a Microservice Architecture (MSA).

The two technologies target different use cases.
However, there is some overlap.

The purpose of this lab is to discover how the two technologies can complement each other.
You do this by applying both a _service mesh_ and an _API Management_ solution to a _cloud native_ application.

=== Background

_Gartner's Magic Quadrant for Full Life Cylce API Management_ puts Red Hat's API management solution in the leader's quadrant.

Red Hat is also a significant innovator in the technologies used to facilitate _Dev Ops_ and _MicroService Architectures_ (MSAs).

With MSAs, the need for a _service mesh_ becomes critical.
As such, Red Hat is contributing to the Istio community project.
In addition, Red Hat is actively working to productize Istio as a supported product on Red Hat's OpenShift Container Platform (OCP).

The following table provides an overview of a feature comparison between API Management and a Service Mesh.

=== Reference

. Istio / Service Mesh

. Distributed Tracing:
.. link:https://github.com/opentracing[Github organization for OpenTracing]
+
Includes the OpenTracing specification as well as the OpenTracing client libraries for many languages.

.. link:https://www.jaegertracing.io/[jaegertracing.io]
.. link:https://github.com/jaegertracing/[Github organization for community Jaeger]
+
Includes source code to community Jaeger as well as Jaeger client libraries for many languages (that implement the corresponding OpenTracing client libraries]

.. link:https://istio.io/blog/2018/v1alpha3-routing/[Overview of Istio v1alpha3 routing API]

=== Alternatives

In regards to API Management and a MSA service mesh, the following are related community initiatives that are not covered in this lab.

==== Standalone community Jaeger

Of related interest is that the upstream community APIcast gateway from Red Hat's 3scale product bundles _OpenTracing_ client library using _Jaeger_.
Subsequently, this allows community apicast to participate in distributed tracing.

Also available in the open-source community is the _all-in-one_ community Jaeger that includes a jaeger-agent, jaeger-collector and jaeger-collector.
link:https://medium.com/@jmprusi_49013/adding-opentracing-support-to-apicast-api-gateway-a8e0a38347d2[This tutorial] very nicely details the use of opentracing enabled apicast with the _all-in-one_ community Jaeger.


As an alternative to the _all-in-one_ standalone Jaeger, Istio also now comes included with Jaeger.
In this lab, this Jaeger based tracing functionality provided by Istio is utilized instead of a standalone Jaeger installation.




==== Istio API Management Working Group

Istio activity is organized into _working groups_.

One of this Istio working groups is on link:https://github.com/istio/community/blob/master/WORKING-GROUPS.md#api-management[API Management].

Members of the  Red Hat engineering team are currently participating in this Istio API Management working group.

Outcomes of this working group may potentially guide the development and roadmap of future releases of the Red Hat 3scale API Management product.

== Lab Asset Overview

=== Environment Variables

Before getting started, you'll want to open a terminal window and set the following environment variables that will be used throughout the duration of this lab.

ifdef::showscript[]
If student lab environment and 3scale tenants were provisioned using the ocp-workload-rhte-mw-api-mesh ansible role, then student details can be found in:

/tmp/3scale_tenants/user_info_file.txt

endif::showscript[]

-----
######  Instructor will provide the values to these environment variables #######

$ export REGION=<provided by your instructor>
$ export GUID=<provided by your instructor>
$ export OCP_PASSWD=<provided by your instructor>
$ export API_ADMIN_ACCESS_TOKEN=<provided by your instructor>



#  Using above variables, copy & paste the following in same terminal #

$ export OCP_WILDCARD_DOMAIN=apps.$REGION.openshift.opentlc.com
$ export OCP_USERNAME=user$GUID
$ export TENANT_NAME=$OCP_USERNAME-3scale-mt


$ export THREESCALE_PORTAL_ENDPOINT=https://${API_ADMIN_ACCESS_TOKEN}@$TENANT_NAME-admin.$OCP_WILDCARD_DOMAIN

$ export OCP_USERNAME=user$GUID
$ export OCP_PROJECT=rhte-mw-api-mesh-$GUID
-----

ifdef::showscript[]

# Potential alternative using service endpoint (may need to use master)
$ export THREESCALE_PORTAL_ENDPOINT=http://${API_ADMIN_ACCESS_TOKEN}@system-provider.3scale-mt-adm0.svc.cluster.local

endif::showscript[]

=== 3scale AMP access

Your lab environment includes access to a multi-tenant 3scale AMP installation.

For the purpose of this lab, you will serve as the administrator of your own 3scale _tenant_ (aka: _domain_)

. Log into the admin portal of your 3scale AMP environment using the information provided to you by your instructor

. To access the admin portal of your 3scale environment, point to your browser to the output of the following:
+
-----
$ echo -en "\n\nhttps://$TENANT_NAME-admin.$OCP_WILDCARD_DOMAIN\n\n"
-----

. Authenticate using the values of $OCP_USERNAME and $OCP_PASSWD   (Your 3scale credentials are the same as your OCP credentials).
+
image::images/3scale_login.png[]


=== OpenShift access

You lab environment is built on Red Hat's OpenShift Container Platform.

Access to your OCP resources can be gained via both the `oc` utility as well as the OCP web console.

. Log into OpenShift
+
-----
$ oc login https://master.$REGION.openshift.opentlc.com -u $OCP_USERNAME -p $OCP_PASSWD
-----

. Ensure that your `oc` client is the same minor release version as the server:
+
-----
$ oc version

oc v3.9.30
kubernetes v1.9.1+a0ce1bc657
features: Basic-Auth GSSAPI Kerberos SPNEGO

Server https://master.a4ec.openshift.opentlc.com:443
openshift v3.9.31
kubernetes v1.9.1+a0ce1bc657
-----

.. In the above example, notice that version of the `oc` client is of the same minor release (v3.9.30) of the OpenShift server (v3.9.31)
.. There a known subtle problems with using a version of the `oc` client that is different from your target OpenShift server.

. View existing projects:
+
-----
$ oc get projects

...

istio-system                                      Active
3scale-mt-adm0           3scale-mt-adm0           Active
rhte-mw-api-mesh-user1   rhte-mw-api-mesh-user1   Active
-----

.. Your OCP user has been provided with _view_ and _edit_ access to the central _istio-system_ namespace with all _control plane_ Istio functionality.
+
Later in this lab, you'll use a utility called _istioctl_ .
This utility will need both view and edit privileges to the _istio-system_ namespace.

.. Your OCP use has also been provided with _view_ access to a multi-tenant

.. The namespace _rhte-mw-api-mesh-*_ is where you will be working throughout the duration of this lab.

. Switch to your  OpenShift project
+
-----
$ oc project $OCP_PROJECT
-----

. Log into OpenShift Web Console
.. Many OpenShift related tasks found in this lab can be completed in the Web Console (as an alternative to using the `oc` utility`.
.. To access, point to your browser to the output of the following:
+
-----
$ echo -en "\n\nhttps://master.$REGION.openshift.opentlc.com\n\n"
-----

.. Authenticate using the values of $OCP_USERNAME and $OCP_PASSWD


[[dvsdc]]
=== Deployment vs DeploymentConfig

Your lab assets consist of a mix of OpenShift _Deployment_ and _DeploymentConfig_ resources.

The _Deployment_ construct is a more recent Kubernetes equivalent of what has always been in OpenShift:  _DeploymentConfig_.

The _istioctl_ utility (introduced later in this lab) of Istio requires the use of the Kubernetes _Deployment_ resource.
Subsequently, for the purpose of this lab, we'll use the Kubernetes _Deployment_ type (instead of DeploymentConfig) for most of the functionality.
One exception to this is the MongoDB.

The CoolStore catalog service included in your lab environment connects to a MongoDB database.
This MongoDB database is managed by Kubernetes using an OpenShift DeploymentConfig instead of a Kubernetes Deployment.
The reason for this is that the OpenShift _DeploymentConfig_ provides more features than a Kubernetes _Deployment_.
In particular, the MongoDB that supports this lab makes use of _life-cycle_ hooks that are only available in a DeploymentConfig.
The life-cycle hooks are used to pre-seed the data in the MongoDB.
This _post deployment_ life-cycle hook is simply ignored if added to a Kubernetes Deployment.


If you interested in learning more about the differences between Kubernetes _Deployments_ and OCP _DeploymentConfigurations_, please see
link:https://docs.openshift.com/container-platform/3.10/dev_guide/deployments/kubernetes_deployments.html#kubernetes-deployments-vs-deployment-configurations[this documentation].

=== CoolStore Catalog Service


==== OpenShift objects

. Review DeploymentConfig
+
-----
$ oc get dc -n $OCP_PROJECT

...

NAME              REVISION   DESIRED   CURRENT   TRIGGERED BY
catalog-mongodb   1          1         1         config,image(mongodb:3.4)
-----

. Review Deployment
+
-----
$ oc get deploy -n $OCP_PROJECT

...

NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
catalog-service   1         1         1            1           4m
-----

. Review running pods
+
-----
$ oc get pods -n $OCP_PROJECT

...

NAME                          READY     STATUS      RESTARTS   AGE
catalog-mongodb-1-clsz4       1/1       Running     0          11m
catalog-service-1-dqb28       1/1       Running     0          11m

...
-----

. Retrieve the URL of the unsecured _catalog_ route:
+
[source,text]
----
$ export NAKED_CATALOG_ROUTE=http://$(oc get route catalog-unsecured -o template --template='{{.spec.host}}' -n $OCP_PROJECT)
----

. Via the catalog route, retrieve the pre-seeded data in the Mongo database:
+
-----
$ curl -X GET "$NAKED_CATALOG_ROUTE/products"
-----
+
.Sample Output
-----
...

{
  "itemId" : "444435",
  "name" : "Oculus Rift",
  "desc" : "The world of gaming has also undergone some very unique and compelling tech advances in recent years. Virtual reality, the concept of complete immersion into a digital universe through a special headset, has been the white whale of gaming and digital technology ever since Nintendo marketed its Virtual Boy gaming system in 1995.",
  "price" : 106.0
}
-----

==== Invoke _Open API Specification_ docs

The link:https://swagger.io/docs/specification/about/[OpenAPI Specification^] (formerly "Swagger Specification") is an API description format for REST APIs. link:https://swagger.io/[Swagger^] is a set of open-source tools built around the OpenAPI specification that can help you design, build, document, and consume REST APIs.

Swagger documentation is available for the REST endpoints of the catalog microservice.

. Display the URL for your project:
+
----
$ echo $NAKED_CATALOG_ROUTE
----

. Copy and paste the URL into a web browser.
* Expect to see the Swagger docs for the REST endpoints:
+
image::images/swagger-ui-coolstore-catalog.png[]

. Click *GET /products Get a list of products* to expand the item.
. Click the *Try it out* button, click *Execute* and view the response.

== Apicast: Plain

In this section of the lab, you will provision a supported version of 3scale _apicast_ gateway to manage your CoolStore _catalog_ service.
Your APIcast will retrive _proxy service_ configurations from the pre-existing 3scale multi-tenant environment.

image::images/deployment_apicast.png[]

The management of this APIcast gateway occurs via a Kubernetes _deployment_ as opposed to an OCP _deploymentconfig_.
The reason for this is discussed previously in the section: <<dvsdc>>.

In a later section of this lab, you will switch to the use of a community variant of APIcast that is enabled with _OpenTracing_ and _Jaeger_ client libraries to participate in distributed tracing.

=== Deploy Apicast

. Create a directory to store files related to this lab:
+
-----
$ mkdir -p $HOME/lab
-----

. Retrieve Apicast template
+
-----
$ curl -o $HOME/lab/3scale-apicast.yml \
          https://raw.githubusercontent.com/gpe-mw-training/3scale_onpremise_implementation_labs/master/resources/rhte/3scale-apicast.yml
-----

. Review Apicast template
+
-----
$ cat $HOME/lab/3scale-apicast.yml | more
-----

. Create Apicast staging related resources in OpenShift:
+
-----
$ oc new-app \
     -f $HOME/lab/3scale-apicast.yml \
     --param THREESCALE_PORTAL_ENDPOINT=$THREESCALE_PORTAL_ENDPOINT \
     --param APP_NAME=stage-apicast \
     --param ROUTE_NAME=catalog-stage-apicast-$OCP_USERNAME \
     --param WILDCARD_DOMAIN=$OCP_WILDCARD_DOMAIN \
     --param THREESCALE_DEPLOYMENT_ENV=sandbox \
     --param APICAST_CONFIGURATION_LOADER=lazy \
     -n $OCP_PROJECT > $HOME/lab/stage-apicast_details.txt
-----

. Create Apicast production related resources in OpenShift:
+
-----
$ oc new-app \
     -f $HOME/lab/3scale-apicast.yml \
     --param THREESCALE_PORTAL_ENDPOINT=$THREESCALE_PORTAL_ENDPOINT \
     --param APP_NAME=prod-apicast \
     --param ROUTE_NAME=catalog-prod-apicast-$OCP_USERNAME \
     --param WILDCARD_DOMAIN=$OCP_WILDCARD_DOMAIN \
     --param THREESCALE_DEPLOYMENT_ENV=production \
     --param APICAST_CONFIGURATION_LOADER=lazy \
     -n $OCP_PROJECT > $HOME/lab/prod-apicast_details.txt
-----

. Resume the paused deploy objects:
+
-----
$ oc rollout resume deploy stage-apicast prod-apicast -n $OCP_PROJECT
-----

=== Configure and Test API Mgmt

In this section of the lab, you can optionally smoke test the management of your _catalog_ RESTful services using your 3scale AMP and APIcast gateways.

Guidance is provided for both experienced and inexperienced 3scale users.
Choose one only.
Afterwards, continue with the section: <<apicast_istio>>

==== Configure & Test: Experienced 3scale users

If you are already proficient with 3scale, then configure and test the management of your _catalog_ RESTful API as per the following :

. Ensure your Apicast gateways started correctly and the value of the _THREESCALE_ENDPOINT_ makes sense.
. Create an API proxy service called _catalog_service_ and configure it to use the APIcast gateway and an API key for security.
. Create an application plan called: _catalog_app_plan_
. Using the existing _Developer_ account and the _catalog_app_plan_, create an application called: _catalog_app_
. Capture the API key for the application and set its value as the following environment variable in your shell terminal:  _CATALOG_USER_KEY_ .
. Configure the _Integration_ section of your _catalog_service_ and publish the service to production.
. Test the _/products_ endpoint of your _catalog_ RESTful service via both your staging and production APIcast gateways.
+
You'll likely want to use the curl utility in a manner similar to the following:
+
-----
$ curl -v -k \
       `echo "https://"$(\
        oc get route/catalog-prod-apicast-$OCP_USERNAME \
        -n $OCP_PROJECT \
        -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----

Proceed to the section: <<apicast_istio>>

==== Configure & Test:  Inexperienced 3scale users

If you are new to API management using 3scale, then follow the instructions found in the <<configuretestapi>> section of the appendix of this lab.

Upon completion, proceed with next section: <<apicast_istio>>.

[[apicast_istio]]
== Apicast: Istio enabled

=== Overview

Your lab environment should now consist of a _catalog_ RESTful service managed by out of the box 3scale AMP 2.2 functionality.

In this section of the lab, you will now inject your APIcast gateway with the _Envoy_ sidecar proxy from Istio.

image::images/deployment_apicast-istio.png[]

In the above diagram, notice the introduction of a new pod: _prod-apicast-istio_.
Ingress requests through the apicast production route and service are now directed to this new apicast pod injected with Istio's envoy sidecar.

The _Envoy_ sidecar in your _prod-apicast-istio_ pod will interoperate with _service mesh control plane_ functionality found in the _istio-system_ namespace.

Your APIcast gateway will continue to pull _service proxy_ configurations from the pre-provisioned multi-tenant 3scale AMP using the value of their  _$THREESCALE_PORTAL_ENDPOINT_ environment variable.

=== Procedure

. Retrieve yaml representation of current apicast production deployment:
+
-----
$ oc get deploy prod-apicast -n $OCP_PROJECT -o yaml > $HOME/lab/prod-apicast.yml
-----

. Differentiate your Istio enabled apicast gateway from your existing APIcast gateway:
+
-----
$ sed -i "s/prod-apicast/$OCP_USERNAME-prod-apicast-istio/" $HOME/lab/prod-apicast.yml
-----
.. The reason you've included $OCP_USERNAME in the name of your istio enabled apicast is because you will need to differentiate with all other istio enabled apicast gateways that may also be managed in the same service mesh.
.. Also, the _observability_ user interfaces included in Istio such as Jaeger are not (currently) multi-tenant.
+
Subsequently, by providing a unique identifer as a prefix to your apicast name, you will be more easily able to identify logs and traces amongst every one else on the system.

. Place the deployment in a paused state:
+
-----
$ sed -i "s/replicas:\ 1/replicas: 1\n  paused: true/" $HOME/lab/prod-apicast.yml
-----

. View configmap in `istio-system` project
+
-----
$ oc describe configmap istio -n istio-system | more
-----
+
Your OCP user has already been enabled with _view_ access on the _istio-system_ namespace.
This provides access to the _istio_ configuration map.
The _istio_ configmap is generated by a cluster-admin when the Istio control plane was installed on OCP.

. Inject Istio configs (from the _istio_ configmap) into a new apicast deployment:
+
-----

$ istioctl kube-inject \
           -f $HOME/lab/prod-apicast.yml \
           > $HOME/lab/prod-apicast-istio.yml
-----

. View Istio injected APIcast gateway deployment descriptor:
+
-----
$ cat $HOME/lab/prod-apicast-istio.yml | more
-----

. Deploy a new Istio enabled apicast production gateway:
+
-----
$ oc create \
     -f $HOME/lab/prod-apicast-istio.yml \
     -n $OCP_PROJECT
-----

. Inject required resource limits and requests into Istio related containers :
+
There is a clusterquota assigned to your OCP user.
This clusterquota requires that all containers (including the _istio-proxy_ and _istio-init_ ) specify _limits_ and _requests_.
+
-----
$ oc patch deploy/$OCP_USERNAME-prod-apicast-istio \
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"istio-proxy", "resources": {   "limits":{"cpu": "500m","memory": "128Mi"},"requests":{"cpu":"50m","memory":"32Mi"}   }}]}}}}'

$ oc patch deploy/$OCP_USERNAME-prod-apicast-istio \
   --patch '{"spec":{"template":{"spec":{"initContainers":[{"name":"istio-init", "resources": {   "limits":{"cpu": "500m","memory": "128Mi"},"requests":{"cpu":"50m","memory":"32Mi"}   }}]}}}}'
-----

. Change _APICAST_LOG_LEVEL_ environment variable to _info_:
+
-----
$ oc patch deploy/$OCP_USERNAME-prod-apicast-istio \
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"'$OCP_USERNAME'-prod-apicast-istio", "env": [{"name":"APICAST_LOG_LEVEL","value":"info" }]}]}}}}'
-----

. Resume the paused deployment:
+
-----
$ oc rollout resume deploy/$OCP_USERNAME-prod-apicast-istio
-----

.. In order for your new istio enabled apicast pod to start, it needs the _anyuid_ SCC.
+
The reason for this is that the _envoy_ side car containers from Istio currently run as a specific userId.
Unlike most middleware containers that can run using any arbitrary userId that is assigned to them at runtime by OCP, the _envoy_ side car containers would immediately fail upon start-up without the _anyuid_ SCC.
You'd see an error similiar to the following:
+
-----
Error creating: pods "customer-7dcd544ff9-" is forbidden: unable to validate against any security context constraint: [spec.initContainers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed capabilities.add: Invalid value: "NET_ADMIN": capability may not be added spec.initContainers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed capabilities.add: Invalid value: "NET_ADMIN":
-----
+
However, for the purpose of this lab, the cluster-admin of your OCP environment previously set the _default_ service account for your OCP project with the _anyuid_ SCC.
This is considered a significant security risk.
A future version of the Red Hat supported Istio will eliminate the need for this _anyuid_ SCC.

.. [red]#TODO Investigate why liveness and readiness probes are failing#
+
-----
Readiness probe failed: Get http://10.1.3.121:8090/status/ready: dial tcp 10.1.3.121:8090: getsockopt: connection refused
Liveness probe failed: Get http://10.1.3.121:8090/status/live: dial tcp 10.1.3.121:8090: getsockopt: connection refused

-----

. Modify _service_ to route to new Istio enabled _apicast_
+
-----
$ oc patch service/prod-apicast \
   --patch '{"spec":{"selector":{"app":"'$OCP_USERNAME'-prod-apicast-istio"}}}'
-----

. Make sure that your `$CATALOG_USER_KEY` environment variable is set:
+
-----
$ echo $CATALOG_USER_KEY

d59904ad4515522ecccb8b81c761a283
-----

. From the terminal, execute the following:
+
-----
$ curl -v -k `echo "https://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -n $OCP_PROJECT -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----
.. The response should actually be a HTTP 404.
.. Why would this be the case ?
... Inspect the APIcast gateway log file for any clues.
... Is the request making it to your new Istio enabled APIcast gateway ?
... The root problem is that your Istio enabled APIcast gateway is unable to connect to the _system-provider_ endpoint exposed by the multi-tenant 3scale AMP via the value of: $THREESCALE_PORTAL_ENDPOINT.
+
Your APICast gateway needs to do this to retrieve all of the policy management configuration data from 3scale AMP.
The reason your APIcast can not make a connection to the 3scale AMP is that $THREESCALE_PORTAL_ENDPOINT references an external internet URL.
By default, Istio blocks all outbound requests to the internet.
In the next section, you'll define an _egress route_ to allow your APIcast gateway to communicate with the 3scale AMP.

=== Apply custom _Egress Route_

In this section, you create a custom Istio _ServiceEntry_ that allows your APIcast gateway to connect to the _backend-listener_ of the multi-tenant 3scale AMP.

. Create a custom Istio _Egress Route_ for Apicast gateway config file:
+
-----
$ echo \
    "apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: $OCP_USERNAME-catalog-apicast-egress-rule
spec:
  hosts:
  - $TENANT_NAME-admin.$OCP_WILDCARD_DOMAIN
  location: MESH_EXTERNAL
  ports:
  - name: https-443
    number: 443
    protocol: HTTPS
  resolution: DNS" \
 > $HOME/lab/catalog-apicast-egressrule.yml
-----

.. Note the value of `spec -> hosts` is set to the same value of the $THREESCALE_PORTAL_ENDPOINT specified in your 3scale apicast gateway.
.. This should allow your apicast gateway to connect to the _system-provider_ service your the multi-tenant 3scale AMP.


. Inject configs from the configmap in _istio-system_ namespace:
+
-----
$ oc create -f $HOME/lab/catalog-apicast-egressrule.yml -n $OCP_PROJECT --as=system:admin
-----
+
WARNING:  Your OCP user has been provided with the ability to _impersonate_ the _system:admin_ user so as to execute this command.
Please use this capability with caution.
In a real-world setting, you would have coordinated with a team-member who does with _cluster admin_ rights to execute this command for you.

. View new ServiceEntry
+
-----
$ oc describe serviceentry $OCP_USERNAME-catalog-apicast-egress-rule --as=system:admin
-----
+
WARNING:  This command also requires _cluster admin_ capabilities to execute.

. Now that a custom _egress route_ has been added, your APIcast should be able to pull configuration data from the 3scale AMP.
+
Use a command like the following to verify that your Istio enabled apicast can now poll the 3scale AMP for proxy service configuration information::
+
-----
$ oc rsh `oc get pod -n $OCP_PROJECT | grep "apicast-istio" | awk '{print $1}'` \
     curl -k ${THREESCALE_PORTAL_ENDPOINT}/admin/api/services.json \
     | python -m json.tool | more

...

{
    "services": [
        {
            "service": {
                "backend_version": "1",
                "created_at": "2018-08-07T11:13:03Z",
                "end_user_registration_required": true,
                "id": 3,
                "links": [
                    {
                        "href": "https://user1-3scale-admin.apps.7777.thinkpadratwater.com/admin/api/services/3/metrics",
                        "rel": "metrics"
                    },


....
-----

. Using the curl utility, re-attempt the request to retrieve catalog data via your istio enabled APIcast gateway .
+
-----
$ curl -v -k `echo "https://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----
+
This time, you should see the catalog data in the response.
This request now flows through your istio enabled APIcast .


== APIcast: OpenTracing enabled


=== Overview

OpenTracing is a consistent, expressive, vendor-neutral API for distributed tracing and context propagation.

Jaeger is one of several implementations of OpenTracing.

The APIcast gateway used in this section of the lab includes a couple of additional libraries:

. /usr/local/openresty/nginx/modules/ngx_http_opentracing_module.so
. /opt/app-root/lib/libjaegertracing.so.0

These library provides support for the _OpenTracing_ specification using _Jaeger_.

image::images/jaeger_architecture.png[]

You'll configure the Opentracing client libraries in your apicast to forward traces via UDP to the _jaeger-agent_.

=== Procedure

. You'll be making quite a few changes to your Istio enabled apicast gateway.  Subsequently, put it in a paused state while those changes are being made:
+
-----
$ oc rollout pause deploy $OCP_USERNAME-prod-apicast-istio
-----

.. Verify that the _jaeger-agent_ exists in the _istio-system_ namespace and is expecting UDP packets on port 6831:
+
-----
$  oc get service jaeger-agent -n istio-system

NAME           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                      AGE
jaeger-agent   ClusterIP   None         <none>        5775/UDP,6831/UDP,6832/UDP   4d
-----
+
The _jaeger-agent_ receives tracing information submitted by jaeger client libraries embedded in apps and forwards in batch to the Jaeger collector.


. Create a json config file that will instruct the opentracing and jaeger related client libraries in the apicast gateway how to push traces to the `jaeger-agent`:
+
-----
$   cat <<EOF > $HOME/lab/jaeger_config.json
{
    "service_name": "$OCP_USERNAME-prod-apicast-istio",
    "disabled": false,
    "sampler": {
      "type": "const",
      "param": 1
    },
    "reporter": {
      "queueSize": 100,
      "bufferFlushInterval": 10,
      "logSpans": false,
      "localAgentHostPort": "jaeger-agent.istio-system:6831"
    },
    "headers": {
      "jaegerDebugHeader": "debug-id",
      "jaegerBaggageHeader": "baggage",
      "TraceContextHeaderName": "uber-trace-id",
      "traceBaggageHeaderPrefix": "testctx-"
    },
    "baggage_restrictions": {
        "denyBaggageOnInitializationFailure": false,
        "hostPort": "jaeger-agent.istio-system:5778",
        "refreshInterval": 60
    }
}
EOF
-----

.. Pay special attention to the value of _localAgentHostPort_ .
+
Thi is the URL that your apicast will push traces (via UDP) to the _jaeger-agent_ service host and port.


. Create a configmap from the opentracing json file:
+
-----
$ oc create configmap jaeger-config --from-file=$HOME/lab/jaeger_config.json -n $OCP_PROJECT
-----

. Mount the configmap to your opentracing enabled apicast:
+
-----
$ oc volume deploy/$OCP_USERNAME-prod-apicast-istio --add -m /tmp/jaeger/ --configmap-name jaeger-config -n $OCP_PROJECT
-----

. Set environment variables that indicate to the apicast where to read opentracing related configurations:
+
-----
$ oc env deploy/$OCP_USERNAME-prod-apicast-istio \
         OPENTRACING_TRACER=jaeger \
         OPENTRACING_CONFIG=/tmp/jaeger/jaeger_config.json \
         -n $OCP_PROJECT
-----

. Update the APIcast _deployment_ to use the Opentracing and Jaeger enabled image:
+
-----
$ oc patch deploy/$OCP_USERNAME-prod-apicast-istio \
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"'$OCP_USERNAME'-prod-apicast-istio", "image": "quay.io/3scale/apicast:master" }]}}}}'
-----

. Resume your Istio and opentracing enabled apicast gateway.
+
-----
$ oc rollout resume deploy $OCP_USERNAME-prod-apicast-istio
-----

. Verify the existence of the opentracing library for NGinx in the APIcast gateway.
+
Once your apicast is back up and running, execute the following command :
+
-----
$ oc rsh `oc get pod | grep "apicast-istio" | awk '{print $1}'` ls -l /usr/local/openresty/nginx/modules/ngx_http_opentracing_module.so

-rwxr-xr-x. 1 root root 1457848 Jun 11 06:29 /usr/local/openresty/nginx/modules/ngx_http_opentracing_module.so
-----

. Verify the existence of the jaeger client library in the apicast gateway:
+
-----
$ oc rsh `oc get pod | grep "apicast-istio" | awk '{print $1}'` ls -l /opt/app-root/lib/libjaegertracing.so.0

lrwxrwxrwx. 1 root root 25 Jun 11 06:38 /opt/app-root/lib/libjaegertracing.so.0 -> libjaegertracing.so.0.3.0
-----


== Jaeger UI


Now that you are using the Opentracing enabled apicast, let's familiarize ourselves the Jaeger user interface to visualize those traces.

=== Overview

Often the first thing to understand about your microservices architecture is specifically which microservices are involved in an end-user transaction.

Istio supports both Zipkin and Jaeger.
For the purpose of this lab, the focus is on Jaeger.

One important term to understand is _span_.
Jaeger defines span as: “a logical unit of work in the system that has an operation name, the start time of the operation, and the duration. Spans can be nested and ordered to model causal relationships.
An RPC call is an example of a span.”

Another important term to understand is _trace_
+
Jaeger defines _trace_ as “adata/execution path through the system, and can be thought of as a directed acyclic graph of spans"

=== Procedure

. Identify the URL to the Jaeger UI:
+
-----
$ echo -en "\n\nhttp://"$(oc get route/tracing -o template --template {{.spec.host}} -n istio-system)"\n\n"
-----
+
Using your browser, navigate to this URL.

. In the _Find Traces_ panel, scroll down to locate the traces associated with your OCP user name:
+
image::images/trace_dropdown_selection.png[]

. Click `Find Traces`.
+
You should see an overview with timeline of all of your traces:
+
image::images/trace_overview.png[]

Traces pertaining to your Istio enabled APIcast gateway are now available .
However, what is missing is tracing that includes the backend _catalog_ service.

In the next section, you'll enable your _catalog_ service to provide tracing data .

== Catalog Service: OpenTracing and Istio enabled

image::images/deployment_catalog-istio.png[]

In the above diagram, notice the introduction of a new pod: _catalog-service-istio_.
Ingress requests through the _catalog-service_ are now directed to this new Istio enabled _catalog_ pod (instead of the original _catalog_ pod that is not Istio enabled).

=== OpenTracing libraries included in _catalog_service

-----
ENV JAEGER_SERVICE_NAME=customer\
  JAEGER_ENDPOINT=http://jaeger-collector.istio-system.svc:14268/api/traces\
  JAEGER_PROPAGATION=b3\
  JAEGER_SAMPLER_TYPE=const\
  JAEGER_SAMPLER_PARAM=1
-----

=== Inject Envoy into _catalog_ service

. Retrieve yaml representation of current _catalog service_ deployment:
+
-----
$ oc get deploy catalog-service -n $OCP_PROJECT -o yaml > $HOME/lab/catalog-service.yml
-----

. Differentiate your Istio enabled catalog service from your existing catalog service:
+
-----
$ sed -i "s/ catalog-service/ $OCP_USERNAME-catalog-service/" $HOME/lab/catalog-service.yml
-----

. Place the deployment in a paused state:
+
-----
$ sed -i "s/replicas:\ 1/replicas: 1\n  paused: true/" $HOME/lab/catalog-service.yml
-----


. Inject Istio configs into a new catalog service deployment
+
-----

$ istioctl kube-inject \
           -f $HOME/lab/catalog-service.yml \
           > $HOME/lab/catalog-service-istio.yml
-----

. View Istio injected catalog service deployment descriptor:
+
-----
$ cat $HOME/lab/catalog-service-istio.yml | more
-----

. Deploy a new Istio enabled apicast production gateway:
+
-----
$ oc create \
     -f $HOME/lab/catalog-service-istio.yml \
     -n $OCP_PROJECT
-----

. Inject required resource limits and requests into Istio related containers :
+
There is a clusterquota assigned to your OCP user.
This clusterquota requires that all containers (including the _istio-proxy_ and _istio-init_ ) specify _limits_ and _requests_.
+
-----
$ oc patch deploy/$OCP_USERNAME-catalog-service \
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"istio-proxy", "resources": {   "limits":{"cpu": "500m","memory": "128Mi"},"requests":{"cpu":"50m","memory":"32Mi"}   }}]}}}}'

$ oc patch deploy/$OCP_USERNAME-catalog-service \
   --patch '{"spec":{"template":{"spec":{"initContainers":[{"name":"istio-init", "resources": {   "limits":{"cpu": "500m","memory": "128Mi"},"requests":{"cpu":"50m","memory":"32Mi"}   }}]}}}}'
-----


. Resume the paused deployment:
+
-----
$ oc rollout resume deploy/$OCP_USERNAME-catalog-service
-----

. Modify the _service_ to route to new Istio enabled _apicast_
+
-----
$ oc patch service/catalog-service \
   --patch '{"spec":{"selector":{"deployment":"'$OCP_USERNAME'-catalog-service"}}}'
-----

. Make sure that your `$CATALOG_USER_KEY` environment variable is set:
+
-----
$ echo $CATALOG_USER_KEY

d59904ad4515522ecccb8b81c761a283
-----

. From the terminal, execute the following:
+
-----
$ curl -v -k `echo "https://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----



== 3scale Analytics

Return back to your 3scale AMP as the domain admin and navigate to the _Analytics_ tab at the top.

image::images/3scale_analytics.png[]

Notice that the _hits_ metric for your _catalog_service_ API is automatically depicted.
3scale analytics can depict the total count of _hits_ on both the API as well as the API method level graphed over time.

Your API analytics are currently course grained in that the _hits_ are the sum of invocations on all methods of your catalog service.
Defining of fine grained _methods_ and _mappings_ for your catalog API will subsequently provide for more fine grained analytics at the method level.

The analytics provided by 3scale compliment the distributed tracing capabilities of Jaeger.

== 3scale mixer adapter

[red]#TO_DO#

The Red Hat 3scale engineering team is actively working on an Istio _mixer_.
This Istio _mixer_ will allow 3scale API Management policies to be applied directly to the _service mesh_ .


== Conclusions

As you know, Openresty is Nginx + luaJIT, and right now, we only get OpenTracing information for the "Nginx" part of it, there aren't any OpenTracing libraries for lua.
We are working on being able to use the OpenTracing C++ libraries from LUA, so we can create spans directly from it, and gain even more visibility into APIcast internals.
For example, this could help debug if that custom policy you just installed is making things slower.


== Questions

TO-DO :  questions to test student knowledge of the concepts / learning objectives of this lab

== Appendix


[[configuretestapi]]
=== Configure and Test API Mgmt

In this section, you define a service that manages access to the Coolstore Catalog service that has already been provisioned for you.

The activities in this section are also found in the pre-req courses but is additionally provided here as a refresher for your conveniance.

==== Define Catalog Service

. From the 3scale AMP Admin Portal home page, navigate to the *API* tab.
. On the far right, click image:images/create_service_icon.png[].
. Enter `catalog_service` for the *Name* and *System Name*.
. Select *NGINX APIcast self-managed* *Gateway* type and not a plugin:
+
image::images/apicast_gw.png[]

. Scroll down the page and for the *Authentication* type, select *API Key (user_key)*:
+
image::images/select_api_key.png[]

. Click *Create Service*.

==== Create Application Plan

Application plans define access policies for your API.

. From the *Overview* page of your new `catalog_service`, scroll to the *Published Application Plans* section.
. Click image:images/create_app_plan_icon.png[]:
+
image::images/create_app_plan.png[]

. Enter `catalog_app_plan` for the *Name* and *System name*:

. Click *Create Application Plan*.

==== Create Application

In this section, you associate an application to an API consumer account.
This generates a _user key_ to the application based on the details previously defined in the application plan.
The user key is used as a query parameter to the HTTP request to invoke your business services via your on-premise APIcast gateway.

. Navigate to the *Developers* tab.
. Select the `Developer` account.
. Create Application
.. Click the *0 Applications* link at the top:
+
NOTE: A default application may have already been created (in which case the link will indicate 1 Application, not 0).
If so, this default application is typically associated with the out-of-the-box `API` service (which is not what you want).
If it exists, feel free to click on default application to identify which service it is associated with and then delete it.

.. Click image:images/create_app_icon.png[].
.. Fill in the *New Application* form as follows:
... *Application plan*: `catalog_app_plan`
... *Service Plan*: `Default`
... *Name*: `catalog_app`
... *Description*: `catalog_app`
+
image::images/create_catalog_app.png[]

.. Click *Create Application*.

. On the details page for your new application (or the default application automatically created), find the API *User Key*:
+
image::images/new_catalog_user_key.png[]

. Create an environment variable set to this user key:
+
-----
$ export CATALOG_USER_KEY=<the catalog app user key>
-----

==== Stage Service Integration

In this section, you define an _API proxy_ to manage your _catalog_ RESTful business service.

. In the 3scale AMP Admin Portal, navigate to the *APIs* tab.
. From your `catalog_service` service, select *Application Plans*.
. For the  `catalog_app_plan` and click the *Publish* link:
+
image::images/publish_app_plan.png[]
. From your `catalog_service` service, select *Integration*.
. Click *Add the base URL of your API and save the configuration*.
+
* This takes you to a page that allows you to associate the apicast staging and production URLs with your new 3scale proxy service.

. Populate the *Configuration: configure & test immediately in the staging environment* form as follows:
.. *Private Base URL*:
... Enter the internal DNS resolvable URL to your Catalog business service.
... The internal URL will be the output of the following:
+
-----
$ echo -en "\n\nhttp://catalog-service.$OCP_PROJECT.svc.cluster.local:8080\n\n"
-----

.. *Staging Public Base URL*: Populate this field with the output from the following command:
+
-----
$ echo -en "\n`oc get route catalog-stage-apicast-$OCP_USERNAME -n $OCP_PROJECT --template "https://{{.spec.host}}"`:443\n\n"
-----

.. *Production Public Base URL*: Populate this field with the output from the following command:
+
-----
$ echo -en "\n`oc get route catalog-prod-apicast-$OCP_USERNAME -n $OCP_PROJECT --template "https://{{.spec.host}}"`:443\n\n"
-----

.. *API test GET request*: Enter `/products`.

** Expect to see a test cURL command populated with the API key assigned to you for the `catalog_app_plan`:
+
image::images/apikey_shows_up.png[]
+
.. If not, go back through the steps to create an Application Plan and corresponding Application.
+
NOTE: When there are multiple developer accounts, Red Hat 3scale API Management uses the default developer account that is created with every new API provider account to determine which user key to use. When creating new services, the 3scale AMP sets the first application from the first account subscribed to the new service as the default.

. Click *Update & test in Staging Environment*
.. In doing so, the `apicast-stage` pod invokes your backend _catalog_ business service as per the `Private Base URL`.
.. The page should turn green with a message indicating success.
+
image::images/stage_success.png[]

. Click *Back to Integration & Configuration*:
. Click *Promote v. 1 to Production*:
+
image::images/stage_and_prod.png[]

Your 3scale by Red Hat service is configured.
Next, the configuration details of your service need to be propagated to your on-premise APIcast gateway.

==== Refresh APIcast at boot
Every time a configuration change is made to an api proxy or application plan, the production APIcast gateways need to be refreshed with the latest changes.

The APIcast gateways are configured to refresh the latest configuration information from the API management platform every 5 minutes.
When this internal NGINX timer is triggered, you see log statements in your APIcast gateway similar to the following:

.Sample Output
-----
[debug] 36#36: *3574 [lua] configuration_loader.lua:132: updated configuration via timer:

....

[info] 36#36: *3574 [lua] configuration_loader.lua:160: auto updating configuration finished successfuly, context: ngx.timer
-----

For the purpose of this lab, instead of potentially waiting for 5 minutes, you can simply bounce your apicast pods .

. Bounce apicast related pods:
+
-----
$ for i in `oc get pod -n $OCP_PROJECT | grep "apicast" | awk '{print $1}'`; do oc delete pod $i; done
-----
+
Kubernetes will detect the absence of these pods and start new ones.
+
Because the value of the _APICAST_CONFIGURATION_LOADER_ environment variable in the pod is set to `boot`, the service proxy configuration from the 3scale AMP will automatically be pulled upon restart.

. Tail the log of the new apicast production pod.

* A debug-level log statement similar to the following appears:
+
.Sample Output
-----
[lua] configuration_store.lua:103: configure(): added service 2555417742084 configuration with hosts: prod-apicast-user1.apps.7777.thinkpadratwater.com, catalog-stage-apicast-user1.apps.7777.thinkpadratwater.com ttl: 300
-----

==== Test Catalog Business Service

In this section, you invoke your Catalog business service via your production APIcast gateway.

. Make sure that your `$CATALOG_USER_KEY` environment variable is still set:
+
-----
$ echo $CATALOG_USER_KEY
-----

. From the terminal, execute the following:
+
-----
$ curl -v -k `echo "https://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----
+
.Sample Output
-----
...

{
  "itemId" : "444435",
  "name" : "Oculus Rift",
  "desc" : "The world of gaming has also undergone some very unique and compelling tech advances in recent years. Virtual reality, the concept of complete immersion into a digital universe through a special headset, has been the white whale of gaming and digital technology ever since Nintendo marketed its Virtual Boy gaming system in 1995.",
  "price" : 106.0
}
-----

. If you are still tailing the log of your `apicast` pod, expect to see statements similar to this:
+
.Sample Output
-----
...

2018/08/06 19:07:46 [info] 24#24: *19 [lua] backend_client.lua:108: authrep(): backend client uri: http://backend-listener.3scale-mt-adm0:3000/transactions/authrep.xml?service_token=a4e0949f1b677611870dab3fb7c142df50871d1eca3d1c9f1615dd514c937df4&service_id=103&usage%5Bhits%5D=1&user_key=ccc4cbae7a44b363a6cd5907a54ff2f9 ok: true status: 200 body:  while sending to client, client: 172.17.0.1, server: _, request: "GET /products?user_key=ccc4cbae7a44b363a6cd5907a54ff2f9 HTTP/1.1", host: "catalog-service.rhte-mw-api-mesh-user1.svc.cluster.local"

...

-----




ifdef::showscript[]

echo -en "\n\ncurl -k ${THREESCALE_PORTAL_ENDPOINT}/admin/api/services.json\n\n"                                    :   test retrival of proxy service info from system-provider

oc rsh `oc get pod | grep "prod-apicast-istio" | awk '{print $1}'` curl localhost:8090/status/live                  :   test liveness probe of istio enabled apicast
oc rsh `oc get pod | grep "prod-apicast-istio" | awk '{print $1}'` curl localhost:8090/status/ready                 :   test readiness probe of istio enabled apicast

oc rsh `oc get pod | grep "apicast-istio" | awk '{print $1}'`                                                       :   ssh into istio enabled apicast gw

oc logs -f  `oc get pod | grep "apicast-istio" \
            | grep "Running" \
            | awk '{print $1}'` -c $OCP_USERNAME-prod-apicast-istio                                                 :   log of istio enabled apicast gw

for i in `oc get pod | grep "apicast-istio" | awk '{print $1}'`; do oc delete pod $i; done                          :   Re-dploy Istio enabled Apicast gateway


TO-DO
  1)  Is a liveness probe necessary for apicast ?  Apicast appears to error out on its own during boot problems.
  2)  With liveness and readiness probes removed, apicast boot error behaves differently depending on whether it is injected with istio
        - istio injected :   apicast boot errors cause fail-over the first 2 or 3 times.  Then no longer any errors.
        - no istio       :   apicast continues to fail upon boot errors

      Turns out envoy proxy is blocking outbound calls at boot for about 1 minute or so
      All outbound calls from primary pods (ie:  apicast invocation to THREESCALE_PORTAL_ENDPOINT and vert.x / fabric8 invocation to kubernetes API to query for configmap) during that time are blocked.

      https://github.com/istio/istio/issues/3533        :   startup time of istio-proxy causes comm issues for up to 30 seconds


  3) investigate istio-ingress
        OCP ha-proxy -> istio-ingress -> apicast gw -> catalog service

  4) when apicast is in info log level, why does it stop rebooting itself when a THREESCALE_PORTAL_ENDPOINT related problem is encountered ?
     when apicast is in debug log level, it continues to cycle when it encounters a THREESCALE_PORTAL_ENDPOINT problem .

  5) with istio injected apicast, boot doesn't start however a curl within the same pod on THREESCALE_PORTAL_ENDPOINT does work

  6) allow user write access to istio-system to allow for execution of:  "istioctl create"

  7) opentracing enabled apicast
        - quay.io/3scale/apicast:master
        - OPENTRACING_TRACER:           Which Tracer implementation to use, right now, only Jaeger is available.
        - OPENTRACING_CONFIG:           Each tracer has a default configuration file, you can see an example here: jaeger.example.json
        - OPENTRACING_HEADER_FORWARD:   By default, uses uber-trace-id, if your OpenTracing has a different configuration, you will need to change this value, if not, ignore it.







The _info_ log level in APIcast gateway actually provides more useful connection error details than does the _debug_ log level.
+
This will become important because we are about to encounter a connection related error now that Istio is introduced .
The connection problem will be in the apicast gateway at boot when it attempts to pull (using the value set in its THREESCALE_PORTAL_ENDPOINT env variable) _proxy-config_ information from the _system-provider_ of 3scale AMP.

. Investigate _apicast_ provisioning problem
+
-----
$ oc logs -f `oc get pod | grep "apicast-istio" | awk '{print $1}'` -c $OCP_USERNAME-prod-apicast-istio

...

2018/08/02 08:32:23 [warn] 23#23: *2 [lua] remote_v2.lua:163: call(): failed to get list of services: invalid status: 0 url: https://user1-3scale-admin.apps.7777.thinkpadratwater.com/admin/api/services.json, context: ngx.timer
2018/08/02 08:32:23 [info] 23#23: *2 [lua] remote_v1.lua:98: call(): configuration request sent: https://user1-3scale-admin.apps.7777.thinkpadratwater.com/admin/api/nginx/spec.json, context: ngx.timer
2018/08/02 08:32:23 [error] 23#23: *2 peer closed connection in SSL handshake, context: ngx.timer
2018/08/02 08:32:23 [warn] 23#23: *2 [lua] remote_v1.lua:108: call(): configuration download error: handshake failed, context: ngx.timer
ERROR: /opt/app-root/src/src/apicast/configuration_loader.lua:57: missing configuration
stack traceback:
	/opt/app-root/src/src/apicast/configuration_loader.lua:57: in function 'boot'
	/opt/app-root/src/libexec/boot.lua:6: in function 'file_gen'
	init_worker_by_lua:49: in function <init_worker_by_lua:47>
	[C]: in function 'xpcall'
	init_worker_by_lua:56: in function <init_worker_by_lua:54>

-----

.. From the log file, notice that initial warning indicates a failure "to get list services" from the 3scale AMP _system-provider_ service.
+
Why would you expect that the _curl_ utility to be able to pull the _service-proxy_ data when rsh'd into the apicast gateway but the apicast gateway itself fails to do so ?



== istio / OCP workshop problem

[2018-08-11 21:02:53.607][154][info][config] external/envoy/source/server/listener_manager_impl.cc:903] all dependencies initialized. starting workers
2018-08-11T21:02:57.106685Z	warn	Epoch 0 terminated with an error: signal: killed
2018-08-11T21:02:57.106713Z	warn	Aborted all epochs
2018-08-11T21:02:57.106739Z	info	Epoch 0: set retry delay to 3.2s, budget to 5
2018-08-11T21:03:00.306904Z	info	Reconciling configuration (budget 5)


=== Lab Focus: Configuration

The emphasis of this lab is on configuration: specifically, configuration of a _Cloud Native _ application managed by 3scale and an Istio  _Service Mesh_.

Students of this lab will not write any business logic.

Development of cloud native applications can be written in a wide variety of development platforms offered by Red Hat to include:

. Red Hat Openshift Application Runtimes (RHOAR)
. Red Hat Fuse on OpenShift

Details about these Red Hat development platforms are out of scope for this specific lab.


endif::showscript[]



ifdef::showscript[]

Which of the following libraries is embedded in community apicast to support distributed tracing?

a) ngx_http_opentracing_module.so
b) libjaegertracing.so.0
c) libzipkintracing.so.0
d) A and B                  *



What is the name of the CustomResourceDefinition introduced by Istio's v1alpha3 routing API that allows for configuration of an egress route?

a) EgressRule
b) DestinationRule
c) ServiceEntry             *
d) EgressRoute



The Jaeger java client library provides which of the following features?

a) Propogation of traces to the jaeger-agent via UDP on port 6831
b) Propogation of traces to the jaeger-collector via TCP by specifying the environment variable: JAEGER_ENDPOINT
c) Setting of the trace sampler type via the environment variable: JAEGER_SAMPLER_TYPE
d) All of the above         *



Which of the following are feature of the Red Hat 3scale product that are not found in Istio ?

a) Developer portal
b) Rate limiting
c) Billing
d) A and C                  *

endif::showscript[]
