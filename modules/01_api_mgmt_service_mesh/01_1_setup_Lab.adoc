:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:
:lab_spreadsheet_apac: link:https://docs.google.com/spreadsheets/d/19Fb4aRYIPWDqUbctXbFvRD7JsT8G_BM9KF5tTo4dWE8/edit?usp=sharing[APAC RHTE: Student lab info spreadsheet]
:lab_spreadsheet: link:https://docs.google.com/spreadsheets/d/1XxwdeGqTSgd1JQssnVMt8TlfyEEPn-MrFz0b2HI_HV0/edit?usp=sharing[EMEA RHTE: Student lab info spreadsheet]

= Lab Setup

.Prerequisites
.. `ssh` utility installed on your laptop.
+
NOTE: If your network is intermittent, then you may want to consider installing the _mosh_ utility (ie: yum install mosh ) as an alternative to the `ssh` utility.

.. Web browser installed on your laptop
.. broadband internet connectivity

:numbered:

== Overview

This first lab orientates you with the course lab assets provided to you.


Your lab environment consists of the following:

. *Remote Virtual Machine*
+
Accessible via the ssh protocol.
It is pre-installed with _OpenShift Container Platform_ and _Istio_.

. *Red Hat 3scale API Manager*
+
Pre-provisioned with a API _domain_ dedicated to you for the duration of this lab.

== Access Course VM via `GUID Grabber`

This section of the lab explains how to access the Red Hat Tech Exchange _GuidGrabber_ in order to obtain a GUID.
This GUID will be used to access a virtual machine that you will use in this course.

. Begin by navigating to: http://bit.ly/rhte-guidgrabber

. From this page select the *Lab Code* :  _A1004 - API Management with 3scale + Istio Microservices_

. Enter the *Activation Key* provided to you by your instructor.

. Click *Next*.

. The resulting page will display your lab's GUID and other useful information about your lab environment.
+
image::images/guid_grabber_response.png[Guid Grabber Information Page]

. Your remote virtual machine is accessible via the `ssh` protocol.
+
Follow the directions exactly as indicated in the Guid Grabber Information Page to ssh into your remote lab VM.

. When you are completely done with your lab environment at the end of this course, please click *Reset Workstation* so that you can move on to the next lab.
If you fail to do this, you will be locked into the GUID from the previous lab.
+
[NOTE]
Clicking *Reset Workstation* will not stop or delete the lab environment.


== Lab Information spreadsheet

The virtual machine that you gained access to in the previous section of this lab is one of two components that comprises your lab environment.

The other component of your lab environment is a 3scale _multi-tenant_ environment that has been pre-provisioned and dedicated to you.

You will select one of those dedicated 3scale _tenants_ as follows:

. Using your browser, navigate to the following spreadsheet: {lab_spreadsheet}.
. Add your name to Column A of the spreadsheet.
. Utilize the values in the corresponding columns B and C to fill out the next section.

== Environment Variables

. Ensure you've ssh'd into your remote lab environment.

ifdef::showscript[]

. Set the following environment variables using the values you've assigned yourself in: {lab_spreadsheet}:
+
-----
$ echo "export API_USERNAME=<column B of spreadsheet>" >> ~/.bashrc
$ echo "export API_ADMIN_ACCESS_TOKEN=<column C of spreadsheet>" >> ~/.bashrc


# Copy and paste the following in the same terminal:
echo 'export API_PASSWD=r3dh4t1!' >> ~/.bashrc
echo 'export OCP_PASSWD=r3dh4t1!' >> ~/.bashrc
echo "export API_TENANT_SUFFIX=3scale-mt-adm1" >> ~/.bashrc
echo "export OCP_USERNAME=developer" >> ~/.bashrc
echo "export API_REGION=4a64" >> ~/.bashrc
echo "export LAB_CODE=a1001" >> ~/.bashrc

-----

endif::showscript[]

. Set the following environment variables using the values you've assigned yourself in: {lab_spreadsheet}:
+
-----
$ echo "export API_USERNAME=<column B of spreadsheet>" >> ~/.bashrc
$ echo "export API_ADMIN_ACCESS_TOKEN=<column C of spreadsheet>" >> ~/.bashrc
$ echo 'export API_PASSWD=<column D of spreadsheet>' >> ~/.bashrc
$ echo 'export OCP_PASSWD=<column E of spreadsheet>' >> ~/.bashrc
$ echo "export API_TENANT_SUFFIX=<column F of spreadsheet>" >> ~/.bashrc
$ echo "export OCP_USERNAME=<column G of spreadsheet>" >> ~/.bashrc
$ echo "export API_REGION=<column H of spreadsheet>" >> ~/.bashrc
$ echo "export LAB_CODE=<column I of spreadsheet>" >> ~/.bashrc
-----



. Copy & paste the following in the same terminal
+
-----

echo "export OCP_REGION=`echo $HOSTNAME | cut -d'.' -f2`" >> ~/.bashrc
echo "export OCP_DOMAIN=clientvm.\$OCP_REGION.rhte.opentlc.com" >> ~/.bashrc
echo "export OCP_WILDCARD_DOMAIN=apps.\$OCP_DOMAIN" >> ~/.bashrc
echo "export MSA_PROJECT=rhte-mw-api-mesh-\$LAB_CODE" >> ~/.bashrc

echo "export API_DOMAIN=\$API_REGION.openshift.opentlc.com" >> ~/.bashrc
echo "export API_WILDCARD_DOMAIN=apps.\$API_DOMAIN" >> ~/.bashrc
echo "export TENANT_NAME=\$API_USERNAME-\$API_TENANT_SUFFIX" >> ~/.bashrc
echo "export THREESCALE_PORTAL_ENDPOINT=https://\${API_ADMIN_ACCESS_TOKEN}@\$TENANT_NAME-admin.\$API_WILDCARD_DOMAIN" >> ~/.bashrc
echo "export BACKEND_ENDPOINT_OVERRIDE=https://backend-\$API_TENANT_SUFFIX.\$API_WILDCARD_DOMAIN" >> ~/.bashrc

-----


. Source the modified ~/.bashrc so that the environment variables are set in your current shell session:
+
-----
$ source ~/.bashrc
-----

== Utilities and resources

. Validate that the following exists in the $PATH of the remote virtual machine:

.. _git_
.. _curl_
.. _sed_
.. _istioctl_
.. _oc_

. Validate that your virtual machine consists of at least 16GB RAM and 4 CPUs.
.. Execute:
+
-----
$ cat /proc/meminfo | grep MemTotal

MemTotal:        16016680 kB
-----

.. Execute:
+
-----
$ cat /proc/cpuinfo | awk '/^processor/{print $3}' | wc -l

4
-----

== OpenShift Container Platform

Your lab environment is built on Red Hat's OpenShift Container Platform.

Access to your OCP resources can be gained via both the `oc` utility as well as the OCP web console.

. Verify that OCP has started:
+
-----
$ sudo systemctl status oc-cluster

...

Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: Server Information ...
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: OpenShift server started.
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: The server is accessible via web console at:
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: https://clientvm.a4f6.rhte.opentlc.com:8443
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: You are logged in as:
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: User:     developer
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: Password: <any value>
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: To login as administrator:
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: oc login -u system:admin
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com systemd[1]: Started OpenShift oc cluster up Service.
-----

. Using the `oc` utility, log into OpenShift
+
-----
$ oc login https://$HOSTNAME:8443 -u $OCP_USERNAME -p $OCP_PASSWD
-----

. Ensure that your `oc` client is the same minor release version as the server:
+
-----
$ oc version

oc v3.10.14
kubernetes v1.10.0+b81c8f8
features: Basic-Auth GSSAPI Kerberos SPNEGO

Server https://master.8091.openshift.opentlc.com:443
openshift v3.10.14
kubernetes v1.10.0+b81c8f8
-----

.. In the above example, notice that version of the `oc` client is of the same release as the remote OCP master API.
.. There are known subtle problems with using a version of the `oc` client that is different from your target OpenShift server.

. View existing projects:
+
-----
$ oc get projects

...

istio-system                                      Active
rhte-mw-api-mesh-1       rhte-mw-api-mesh-1       Active
-----

.. *istio-system*
+
Your OCP user has been provided with _view_ and _edit_ access to the central _istio-system_ namespace with all _control plane_ Istio functionality.
+
Later in this lab, you'll use a utility called _istioctl_ .
This utility will need both view and edit privileges to the _istio-system_ namespace.

.. *rhte-mw-api-mesh-**
+
The namespace _rhte-mw-api-mesh-*_ is where you will be working throughout the duration of this lab.

. Switch to your  OpenShift project
+
-----
$ oc project $MSA_PROJECT
-----

. View details of the ClusterQuota that the _cluster-admin_ has assigned to your openshift user:
+
-----
$ oc describe AppliedClusterResourceQuota clusterquota-rhte-mw-api-mesh-developer

....

Resource                Used    Hard
--------                ----    ----
configmaps              1       20
limits.cpu              1100m   10
limits.memory           1780Mi  15Gi
persistentvolumeclaims  1       20
pods                    4       30
requests.cpu            425m    5
requests.memory         820Mi   6Gi
requests.storage        1Gi     50Gi
secrets                 24      150
services                4       150
-----

. Validate the ability to _impersonate_ cluster admin:
+
-----
$ oc get nodes --as=system:admin

NAME        STATUS    ROLES     AGE       VERSION
localhost   Ready     <none>    16h       v1.10.0+b81c8f8
-----
+
For the purpose of this lab, the cluster-admin of your OCP environment has provided you with the ability to _impersonate_ the _cluster-admin_.
You would not have had the ability to execute the above command (by specifying `--as=system:admin`) if the cluster-admin had not already done so.
From time to time, you will make use of this ability to impersonate cluster admin in the next lab.

. Log into OpenShift Web Console
.. Many OpenShift related tasks found in this lab can be completed in the Web Console (as an alternative to using the `oc` utility.
.. To access the OCP web console, point to your browser to the output of the following:
+
-----
$ echo -en "\n\nhttps://$OCP_DOMAIN:8443\n\n"
-----

.. Authenticate using the values of $OCP_USERNAME and $OCP_PASSWD


== Catalog Service

The backend business service used throughout this course will be a simple application called the `Catalog Service`.
In this section of the lab, you review this pre-provisioned `Catalog Service`.

[[dvsdc]]
=== Deployment vs DeploymentConfig

Your lab assets consist of a mix of OpenShift _Deployment_ and _DeploymentConfig_ resources.

The _Deployment_ construct is a more recent Kubernetes equivalent of what has always been in OpenShift:  _DeploymentConfig_.

The _istioctl_ utility (introduced later in this lab) of Istio requires the use of the Kubernetes _Deployment_ resource.
Subsequently, for the purpose of this lab, we'll use the Kubernetes _Deployment_ type (instead of DeploymentConfig) for most of the functionality.
One exception to this is the MongoDB.

The CoolStore catalog service included in your lab environment connects to a MongoDB database.
This MongoDB database is managed by Kubernetes using an OpenShift DeploymentConfig instead of a Kubernetes Deployment.
The reason for this is that the OpenShift _DeploymentConfig_ provides more features than a Kubernetes _Deployment_.
In particular, the MongoDB that supports this lab makes use of _life-cycle_ hooks that are only available in a DeploymentConfig.
The life-cycle hooks are used to pre-seed the data in the MongoDB.
This _post deployment_ life-cycle hook is simply ignored if added to a Kubernetes Deployment.


If you interested in learning more about the differences between Kubernetes _Deployments_ and OCP _DeploymentConfigurations_, please see
link:https://docs.openshift.com/container-platform/3.10/dev_guide/deployments/kubernetes_deployments.html#kubernetes-deployments-vs-deployment-configurations[this documentation].

==== OpenShift objects

. Review DeploymentConfig
+
-----
$ oc get dc -n $MSA_PROJECT

...

NAME              REVISION   DESIRED   CURRENT   TRIGGERED BY
catalog-mongodb   1          1         1         config,image(mongodb:3.4)
-----

. Review Deployment
+
-----
$ oc get deploy -n $MSA_PROJECT

...

NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
catalog-service   1         1         1            1           4m
-----

. Review running pods
+
-----
$ oc get pods -n $MSA_PROJECT

...

NAME                          READY     STATUS      RESTARTS   AGE
catalog-mongodb-1-clsz4       1/1       Running     0          11m
catalog-service-1-dqb28       1/1       Running     0          11m

...
-----

. Retrieve the URL of the unsecured _catalog_ route:
+
----
$ echo "export NAKED_CATALOG_ROUTE=$(oc get route catalog-unsecured -o template --template='{{.spec.host}}' -n $MSA_PROJECT)" >> ~/.bashrc

$ source ~/.bashrc
----
+
NOTE:  You'll use $NAKED_CATALOG_ROUTE environment variable a various stages in the lab.

. Via the catalog route, retrieve the pre-seeded data in the Mongo database:
+
-----
$ curl -X GET "http://$NAKED_CATALOG_ROUTE/products"




...


{
  "itemId" : "444435",
  "name" : "Oculus Rift",
  "desc" : "The world of gaming has also undergone some very unique and compelling tech advances in recent years. Virtual reality, the concept of complete immersion into a digital universe through a special headset, has been the white whale of gaming and digital technology ever since Nintendo marketed its Virtual Boy gaming system in 1995.",
  "price" : 106.0
}
-----

==== Invoke _Open API Specification_ docs

The link:https://swagger.io/docs/specification/about/[OpenAPI Specification^] (formerly "Swagger Specification") is an API description format for REST APIs. link:https://swagger.io/[Swagger^] is a set of open-source tools built around the OpenAPI specification that can help you design, build, document, and consume REST APIs.

Swagger documentation is available for the REST endpoints of the catalog microservice.
You can optionally view this documentation as follows:

. Display the URL for your project:
+
----
$ echo "http://$NAKED_CATALOG_ROUTE"
----

. Copy and paste the URL into a web browser.
* Expect to see the Swagger docs for the REST endpoints:
+
image::images/swagger-ui-coolstore-catalog.png[]

. Click *GET /products Get a list of products* to expand the item.
. Click the *Try it out* button, click *Execute* and view the response.

== API Manager

Your lab environment includes access to a multi-tenant API Manager installation.

For the purpose of this lab, you will serve as the administrator of your own 3scale _tenant_ (aka: _domain_)

Log into the admin portal of your API Manager environment as follows:

. To access the admin portal of your 3scale environment, point to your browser to the output of the following:
+
-----
$ echo -en "\n\nhttps://$TENANT_NAME-admin.$API_WILDCARD_DOMAIN\n\n"
-----

. Authenticate using the values of $API_USERNAME and $API_PASSWD .
+
image::images/3scale_login.png[]

== Appendix

=== Optional:  Administrative Access

. On your remote lab environment, you can optionally gain access to the `root` operating system user by executing: `sudo -i`
. As the `root` operating system user, `cluster admin` access to your OCP environment can be achieved by executing :
+
-----
# oc login -u system:admin
-----

. You can check the status of the OCP system service by executing:
+
-----
# systemctl status oc-cluster.service
-----

. The OCP environment can be restarted as follows:
+
-----
# systemctl restart oc-cluster.service
-----

. You can optionally install additional networking utilities (that could serve as useful troubleshooting tools ) as follows:
+
-----
$ yum install telnet bind-utils
-----

=== Optional:  Lab Environment Provisioning

This section is offered to those that are interested in setting up an environment to support this lab using their own resources.

==== Ansible Roles

The lab environment can be provisioned via the following ansible roles:

. *ocp-workload-3scale-multitenant*
+
The link:https://github.com/sborenst/ansible_agnostic_deployer/tree/development/ansible/roles/ocp-workload-3scale-multitenant[ocp-workload-3scale-multitenant] ansible role will provision a multi-tenant 3scale API Manager.
+
This role only needs to be executed one time (so as to provision only one multi-tenant API Manager) on a pre-existing OCP 3.10 environment.
+
The role also offers the ability to provision a configurable number of _tenants_ in that multi-tenant API Manager environment.
And, if provisioning tenants, the role also provides the ability to automatically provision API gateways for each tenant (co-located in the same OCP cluster as the API Manager but in their own namespaces).

. *ocp-workload-istio-community*
+
The link:https://github.com/sborenst/ansible_agnostic_deployer/tree/development/ansible/roles/ocp-workload-istio-community[ocp-workload-istio-community] ansible role will layer Istio on a a pre-existing OCP 3.10 environment.
+
This role should be executed on an OCP environment dedicated to a student (ie:  using oc cluster up ).
This role is applied to the same OCP environment utilized by the _ocp-workload-rhte-mw-api-mesh_ role.

. *ocp-workload-rhte-mw-api-mesh*
+
The link:https://github.com/sborenst/ansible_agnostic_deployer/tree/development/ansible/roles/ocp-workload-rhte-mw-api-mesh[ocp-workload-rhte-mw-api-mesh] ansible role will provision supporting lab assets (ie: the catalog service).
+
This role should be executed on an OCP environment dedicated to a student (ie:  using oc cluster up ).
This role is applied to the same OCP environment utilized by the _ocp-workload-istio-community_ role.

