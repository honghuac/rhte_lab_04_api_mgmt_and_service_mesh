:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= API Mgmt and Service Mesh Lab

.Goals
* Inject Istio Envoy proxy configs into an API gateway
* Configure an Istio Egress Route for an API gateway
* End-to-end distributed tracing of a MSA application using Jaeger implementation of the _OpenTracing_ specification

.Prerequisite
* Skills
** Completion of the _APIs as a Business_ lab
** Completion of the _MSA and Service Mesh_ lab
* Tools
** `curl` utility
** `sed` utility
** `istioctl` utility
** `oc` version 3.9 utility

:numbered:

== Overview

_API Management_ facilitates relationships between API consumers and producers.
It lowers the link:https://en.wikipedia.org/wiki/Transaction_cost[transaction costs] (ie: search costs, price discovery, policing and enforcement costs, etc ) that would otherwise hinder mutually beneficial exchange between API consumer and producer.
It is often the case that these API producers and consumers do not personally know each other.

A _service mesh_ provides the technical resilience and _observability_ needed to facilitate a Microservice Architecture (MSA).

The two technologies target different use cases.
However, there is some overlap.

The purpose of this lab is to discover how the two technologies can complement each other.
You do this by applying both a _service mesh_ and an _API Management_ solution to a _cloud native_ application.

=== Background

_Gartner's Magic Quadrant for Full Life Cylce API Management_ puts Red Hat's API management solution in the leader's quadrant.

Red Hat is also a significant innovator in the technologies used to facilitate _Dev Ops_ and _MicroService Architectures_ (MSAs).

With MSAs, the need for a _service mesh_ becomes critical.
As such, Red Hat is contributing to the Istio community project.
In addition, Red Hat is actively working to productize Istio as a supported product on Red Hat's OpenShift Container Platform (OCP).

The following table provides an overview of a feature comparison between API Management and a Service Mesh.

=== Reference

. Istio / Service Mesh

. Distributed Tracing:
.. link:https://github.com/opentracing[Github organization for OpenTracing]
+
Includes the OpenTracing specification as well as the OpenTracing client libraries for many languages.

.. link:https://www.jaegertracing.io/[jaegertracing.io]
.. link:https://github.com/jaegertracing/[Github organization for community Jaeger]
+
Includes source code to community Jaeger as well as Jaeger client libraries for many languages (that implement the corresponding OpenTracing client libraries]

.. link:https://istio.io/blog/2018/v1alpha3-routing/[Overview of Istio v1alpha3 routing API]

=== Alternatives

In regards to API Management and a MSA service mesh, the following are related community initiatives that are not covered in this lab.

==== Standalone community Jaeger

Of related interest is that the upstream community API gateway from Red Hat's 3scale product bundles _OpenTracing_ client library using _Jaeger_.
Subsequently, this allows community API gateway to participate in distributed tracing.

Also available in the open-source community is the _all-in-one_ community Jaeger that includes a jaeger-agent, jaeger-collector and jaeger-collector.
link:https://medium.com/@jmprusi_49013/adding-opentracing-support-to-apicast-api-gateway-a8e0a38347d2[This tutorial] very nicely details the use of opentracing enabled API gateway with the _all-in-one_ community Jaeger.


As an alternative to the _all-in-one_ standalone Jaeger, Istio also now comes included with Jaeger.
In this lab, this Jaeger based tracing functionality provided by Istio is utilized instead of a standalone Jaeger installation.




==== Istio API Management Working Group

Istio activity is organized into _working groups_.

One of this Istio working groups is on link:https://github.com/istio/community/blob/master/WORKING-GROUPS.md#api-management[API Management].

Members of the  Red Hat engineering team are currently participating in this Istio API Management working group.

Outcomes of this working group may potentially guide the development and roadmap of future releases of the Red Hat 3scale API Management product.

In the future, this lab may demonstrate initiatives that may come out of this Istio API Management Working Group.

== Apicast: Plain

In this section of the lab, you will provision a supported version of 3scale _apicast_ gateway to manage your CoolStore _catalog_ service.
Your API gateway will retrive _proxy service_ configurations from the pre-existing 3scale multi-tenant environment.

image::images/deployment_apicast.png[]

The management of this API gateway occurs via a Kubernetes _deployment_ as opposed to an OCP _deploymentconfig_.
The reason for this is discussed previously in the section: <<dvsdc>>.

In a later section of this lab, you will switch to the use of a community variant of API gateway that is enabled with _OpenTracing_ and _Jaeger_ client libraries to participate in distributed tracing.


=== Deploy Apicast

. Create a directory to store files related to this lab:
+
-----
$ mkdir -p $HOME/lab
-----

. Retrieve API gateway template
+
-----
$ curl -o $HOME/lab/3scale-apicast.yml \
          https://raw.githubusercontent.com/gpe-mw-training/3scale_onpremise_implementation_labs/master/resources/rhte/3scale-apicast.yml
-----

. Review API gateway template
+
-----
$ cat $HOME/lab/3scale-apicast.yml | more
-----

. Create API gateway staging related resources in OpenShift:
+
-----
$ oc new-app \
     -f $HOME/lab/3scale-apicast.yml \
     --param THREESCALE_PORTAL_ENDPOINT=$THREESCALE_PORTAL_ENDPOINT \
     --param APP_NAME=stage-apicast \
     --param ROUTE_NAME=catalog-stage-apicast-$OCP_USERNAME \
     --param WILDCARD_DOMAIN=$OCP_WILDCARD_DOMAIN \
     --param THREESCALE_DEPLOYMENT_ENV=sandbox \
     --param APICAST_CONFIGURATION_LOADER=lazy \
     -n $MSA_PROJECT > $HOME/lab/stage-apicast_details.txt
-----

. Create API gateway production related resources in OpenShift:
+
-----
$ oc new-app \
     -f $HOME/lab/3scale-apicast.yml \
     --param THREESCALE_PORTAL_ENDPOINT=$THREESCALE_PORTAL_ENDPOINT \
     --param APP_NAME=prod-apicast \
     --param ROUTE_NAME=catalog-prod-apicast-$OCP_USERNAME \
     --param WILDCARD_DOMAIN=$OCP_WILDCARD_DOMAIN \
     --param THREESCALE_DEPLOYMENT_ENV=production \
     --param APICAST_CONFIGURATION_LOADER=lazy \
     -n $MSA_PROJECT > $HOME/lab/prod-apicast_details.txt
-----

. Resume the paused deploy objects:
+
-----
$ oc rollout resume deploy stage-apicast prod-apicast -n $MSA_PROJECT
-----

=== Configure and Test API Mgmt

In this section of the lab, you can optionally smoke test the management of your _catalog_ RESTful services using your API Manager and API gateways.

Guidance is provided for both experienced and inexperienced 3scale users.
Choose one only.
Afterwards, continue with the section: <<apicast_istio>>

==== Configure & Test: Experienced 3scale users

If you are already proficient with 3scale, then configure and test the management of your _catalog_ RESTful API as per the following :

. Ensure your API gateways started correctly and the value of the _THREESCALE_ENDPOINT_ makes sense.
. Create an API proxy service called _catalog_service_ and configure it to use the API gateway and an API key for security.
. Create an application plan called: _catalog_app_plan_
. Using the existing _Developer_ account and the _catalog_app_plan_, create an application called: _catalog_app_
. Capture the API key for the application and set its value as the following environment variable in your shell terminal:  _CATALOG_USER_KEY_ .
. Configure the _Integration_ section of your _catalog_service_ and publish the service to production.
. Test the _/products_ endpoint of your _catalog_ RESTful service via both your staging and production API gateways.
+
You'll likely want to use the curl utility in a manner similar to the following:
+
-----
$ curl -v -k \
       `echo "https://"$(\
        oc get route/catalog-prod-apicast-$OCP_USERNAME \
        -n $MSA_PROJECT \
        -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----
+
WARNING: [red]#Don't proceed beyond this section until this smoke test of your non istio enabled environment passes and the response from your production API gateway is a listing of catalog data#

Once you've smoke tested your API managed environment, proceed to the section: <<apicast_istio>>

==== Configure & Test:  Inexperienced 3scale users

If you are new to API management using 3scale, then follow the instructions found in the <<configuretestapi>> section of the appendix of this lab.

Upon completion, proceed with next section: <<apicast_istio>>.

[[apicast_istio]]
== Apicast: Istio enabled

=== Overview

Your lab environment should now consist of a _catalog_ RESTful service managed by out of the box API Manager 2.2 functionality.

In this section of the lab, you will now inject your API gateway with the _Envoy_ sidecar proxy from Istio.

image::images/deployment_apicast-istio.png[]

In the above diagram, notice the introduction of a new pod: _prod-apicast-istio_.
Ingress requests through the API gateway production route and service are now directed to this new API gateway pod injected with Istio's envoy sidecar.

The _Envoy_ sidecar in your _prod-apicast-istio_ pod will interoperate with _service mesh control plane_ functionality found in the _istio-system_ namespace.

Your API gateway will continue to pull _service proxy_ configurations from the pre-provisioned multi-tenant API Manager using the value of their  _$THREESCALE_PORTAL_ENDPOINT_ environment variable.

=== Procedure

. Retrieve yaml representation of current API gateway production deployment:
+
-----
$ oc get deploy prod-apicast -n $MSA_PROJECT -o yaml > $HOME/lab/prod-apicast.yml
-----

. Differentiate your Istio enabled API gateway from your existing API gateway:
+
-----
$ sed -i "s/prod-apicast/$OCP_USERNAME-prod-apicast-istio/" $HOME/lab/prod-apicast.yml
-----
.. The reason you've included $OCP_USERNAME in the name of your istio enabled API gateway is because you will need to differentiate with all other istio enabled API gateways that may also be managed in the same service mesh.
.. Also, the _observability_ user interfaces included in Istio such as Jaeger are not (currently) multi-tenant.
+
Subsequently, by providing a unique identifer as a prefix to your API gateway name, you will be more easily able to identify logs and traces amongst every one else on the system.

. Place the deployment in a paused state:
+
-----
$ sed -i "s/replicas:\ 1/replicas: 1\n  paused: true/" $HOME/lab/prod-apicast.yml
-----

. View configmap in `istio-system` project
+
-----
$ oc describe configmap istio -n istio-system | more
-----
+
Your OCP user has already been enabled with _view_ access on the _istio-system_ namespace.
This provides access to the _istio_ configuration map.
The _istio_ configmap is generated by a cluster-admin when the Istio control plane was installed on OCP.

. Inject Istio configs (from the _istio_ configmap) into a new API gateway deployment:
+
-----

$ istioctl kube-inject \
           -f $HOME/lab/prod-apicast.yml \
           > $HOME/lab/prod-apicast-istio.yml
-----

. View Istio injected API gateway deployment descriptor:
+
-----
$ cat $HOME/lab/prod-apicast-istio.yml | more
-----

. Deploy a new Istio enabled API gateway production gateway:
+
-----
$ oc create \
     -f $HOME/lab/prod-apicast-istio.yml \
     -n $MSA_PROJECT
-----

. Inject required resource limits and requests into Istio related containers :
+
There is a clusterquota assigned to your OCP user.
This clusterquota requires that all containers (including the _istio-proxy_ and _istio-init_ ) specify _limits_ and _requests_.
+
-----
$ oc patch deploy/$OCP_USERNAME-prod-apicast-istio \
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"istio-proxy", "resources": {   "limits":{"cpu": "500m","memory": "128Mi"},"requests":{"cpu":"50m","memory":"32Mi"}   }}]}}}}'

$ oc patch deploy/$OCP_USERNAME-prod-apicast-istio \
   --patch '{"spec":{"template":{"spec":{"initContainers":[{"name":"istio-init", "resources": {   "limits":{"cpu": "500m","memory": "128Mi"},"requests":{"cpu":"50m","memory":"32Mi"}   }}]}}}}'
-----

. Change _APICAST_LOG_LEVEL_ environment variable to _info_:
+
-----
$ oc patch deploy/$OCP_USERNAME-prod-apicast-istio \
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"'$OCP_USERNAME'-prod-apicast-istio", "env": [{"name":"APICAST_LOG_LEVEL","value":"info" }]}]}}}}'
-----

. Resume the paused deployment:
+
-----
$ oc rollout resume deploy/$OCP_USERNAME-prod-apicast-istio
-----

.. Notice the presence of an additional container in your new pod.  This additional container is the istio proxy sidecar.
.. Both containers in the new pod should have started and the pod should be in a _Running_ state:
+
-----
NAME                                         READY     STATUS    RESTARTS   AGE
catalog-mongodb-1-hfwqw                      1/1       Running   0          57m
catalog-service-5558855cfd-wwrth             1/1       Running   0          56m
prod-apicast-57db66b8b7-fm268                1/1       Running   0          28m
stage-apicast-7f9d46f6dc-vtkrr               1/1       Running   0          43m


user50-prod-apicast-istio-784dc96c75-gvh5f   2/2       Running   0          5m

-----
+
If either of the containers did not start up and the _READY_ column indicates anything other than _2/2_, then this an indication of a problem.
It's likely that _liveness_ and/or _readiness_ probes on the API gateway are failing.
It's possible that this is due to a mis-configuration of Istio.
As an initial troubleshooting step, remove the _liveness_ and _readiness_ probes defined in the deployment.
After doing so, do both containers start ?
Talk to your instructor about additional steps to troubleshoot the likely Istio related problem(s).

.. In order for your new istio enabled API gateway pod to start, it needs the _anyuid_ SCC.
+
The reason for this is that the _envoy_ side car containers from Istio currently run as a specific userId.
Unlike most middleware containers that can run using any arbitrary userId that is assigned to them at runtime by OCP, the _envoy_ side car containers would immediately fail upon start-up without the _anyuid_ SCC.
You'd see an error similiar to the following:
+
-----
Error creating: pods "catalog-7dcd544ff9-" is forbidden: unable to validate against any security context constraint: [spec.initContainers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed capabilities.add: Invalid value: "NET_ADMIN": capability may not be added spec.initContainers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed capabilities.add: Invalid value: "NET_ADMIN":
-----
+
However, for the purpose of this lab, the cluster-admin of your OCP environment previously set the _default_ service account for your OCP project with the _anyuid_ SCC.
This is considered a significant security risk.
A future version of the Red Hat supported Istio will eliminate the need for this _anyuid_ SCC.



. Modify the _prod-apicast_ service to route to new Istio enabled _apicast_
+
-----
$ oc patch service/prod-apicast \
   --patch '{"spec":{"selector":{"app":"'$OCP_USERNAME'-prod-apicast-istio"}}}'
-----

. Make sure that your `$CATALOG_USER_KEY` environment variable is set:
+
-----
$ echo $CATALOG_USER_KEY

d59904ad4515522ecccb8b81c761a283
-----

. From the terminal, execute the following:
+
-----
$ curl -v -k `echo "https://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -n $MSA_PROJECT -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----
.. The response should actually be a HTTP 404.
.. Why would this be the case ?
... Inspect the API gateway log file for any clues.
... Is the request making it to your new Istio enabled API gateway ?
... The root problem is that your Istio enabled API gateway is unable to connect to the _system-provider_ endpoint exposed by the multi-tenant API Manager via the value of: $THREESCALE_PORTAL_ENDPOINT.
+
Your API gateway needs to do this to retrieve all of the policy management configuration data from the API Manager.
The reason your API gateway can not make a connection to the 3scale API Manager is that $THREESCALE_PORTAL_ENDPOINT references an external internet URL.
By default, Istio blocks all outbound requests to the internet.
In the next section, you'll define an _egress route_ to allow your API gateway to communicate with the API Manager.

.  Isolate the problem with your new Istio enabled API gateway by testing the call to the _system-provider_ from within the API gateway.
+
Use a command such as the following:
+
-----
$ oc rsh `oc get pod -n $MSA_PROJECT | grep "apicast-istio" | awk '{print $1}'` \
          curl -v -k ${THREESCALE_PORTAL_ENDPOINT}/admin/api/services.json


...

Defaulting container name to user50-prod-apicast-istio.
Use 'oc describe pod/user50-prod-apicast-istio-784dc96c75-vxxz5 -n rhte-mw-api-mesh-50' to see all of the containers in this pod.
* About to connect() to user50-3scale-mt-admin.apps.8091.openshift.opentlc.com port 443 (#0)
*   Trying 52.7.161.237...
* Connected to user50-3scale-mt-admin.apps.8091.openshift.opentlc.com (52.7.161.237) port 443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* NSS error -5938 (PR_END_OF_FILE_ERROR)
* Encountered end of file
* Closing connection 0
curl: (35) Encountered end of file
command terminated with exit code 35
-----

=== Apply custom _Egress Route_

In this section, you create a custom Istio _ServiceEntry_ that allows your API gateway to connect to the _backend-listener_ of the multi-tenant API Manager.

. Create a custom Istio _Egress Route_ for API gateway config file:
+
-----
$ echo \
    "apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: $OCP_USERNAME-catalog-apicast-egress-rule
spec:
  hosts:
  - $TENANT_NAME-admin.$OCP_WILDCARD_DOMAIN
  location: MESH_EXTERNAL
  ports:
  - name: https-443
    number: 443
    protocol: HTTPS
  resolution: DNS" \
 > $HOME/lab/catalog-apicast-egressrule.yml
-----

.. Note the value of `spec -> hosts` is set to the same value of the $THREESCALE_PORTAL_ENDPOINT specified in your 3scale API gateway.
.. This should allow your API gateway to connect to the route that exposes the _system-provider_ service of the multi-tenant API Manager.


. Load the new egress rule:
+
-----
$ oc create -f $HOME/lab/catalog-apicast-egressrule.yml -n $MSA_PROJECT --as=system:admin
-----
+
WARNING:  Your OCP user has been provided with the ability to _impersonate_ the _system:admin_ user so as to execute this command.
Please use this capability with caution.
In a real-world setting, you would have coordinated with a team-member who does with _cluster admin_ rights to execute this command for you.

. View new ServiceEntry
+
-----
$ oc describe serviceentry $OCP_USERNAME-catalog-apicast-egress-rule --as=system:admin
-----
+
WARNING:  This command also requires _cluster admin_ capabilities to execute.

. Now that a custom _egress route_ has been added, your API gateway should be able to pull configuration data from the API Manager.
+
Use a command like the following to verify that your Istio enabled API gateway can now poll the API Manager for proxy service configuration information::
+
-----
$ oc rsh `oc get pod -n $MSA_PROJECT | grep "apicast-istio" | awk '{print $1}'` \
     curl -k ${THREESCALE_PORTAL_ENDPOINT}/admin/api/services.json \
     | python -m json.tool | more

...

{
    "services": [
        {
            "service": {
                "backend_version": "1",
                "created_at": "2018-08-07T11:13:03Z",
                "end_user_registration_required": true,
                "id": 3,
                "links": [
                    {
                        "href": "https://user1-3scale-admin.apps.7777.thinkpadratwater.com/admin/api/services/3/metrics",
                        "rel": "metrics"
                    },


....
-----

. Either wait up to 5 minutes for your Istio enabled API gateway to refresh its proxy configuration (because pulling this config data previously failed) or bounce the pod.
. Using the curl utility, re-attempt the request to retrieve catalog data via your istio enabled API gateway .
+
-----
$ curl -v -k `echo "https://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----
+
This time, you should see the catalog data in the response.
This request now flows through your istio enabled API gateway .

[blue]#Although at this point nothing has changed from a functional perspective in your lab, you are now well on your way toward accomplishing a non-functional objective of this lab: apply a service mesh to your API managed application.#


== APIcast: OpenTracing enabled


=== Overview

OpenTracing is a consistent, expressive, vendor-neutral API for distributed tracing and context propagation.

Jaeger is one of several implementations of OpenTracing.

The API gateway that you will switch to in this section of the lab includes a couple of additional _Opentracing_ and _Jaeger_ related libraries:

. *ngx_http_opentracing_module_so*
+
Located at the following path in the API gateway: /usr/local/openresty/nginx/modules/ngx_http_opentracing_module.so

. *libjaegertracing.so.0*
+
Located at the following path in the API gateway: /opt/app-root/lib/libjaegertracing.so.0

These libraries provide support for the _OpenTracing_ specification using _Jaeger_.

image::images/jaeger_architecture.png[]

You'll configure the Opentracing client libraries in your API gateway to forward traces via UDP to the _jaeger-agent_.

=== Procedure

. You'll be making quite a few changes to your Istio enabled API gateway.  Subsequently, put it in a paused state while those changes are being made:
+
-----
$ oc rollout pause deploy $OCP_USERNAME-prod-apicast-istio
-----

.. Verify that the _jaeger-agent_ exists in the _istio-system_ namespace and is expecting UDP packets on port 6831:
+
-----
$  oc get service jaeger-agent -n istio-system

NAME           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                      AGE
jaeger-agent   ClusterIP   None         <none>        5775/UDP,6831/UDP,6832/UDP   4d
-----
+
The _jaeger-agent_ receives tracing information submitted by jaeger client libraries embedded in apps and forwards in batch to the Jaeger collector.


. Create a json config file that will instruct the opentracing and jaeger related client libraries in the API gateway how to push traces to the `jaeger-agent`:
+
-----
$   cat <<EOF > $HOME/lab/jaeger_config.json
{
    "service_name": "$OCP_USERNAME-prod-apicast-istio",
    "disabled": false,
    "sampler": {
      "type": "const",
      "param": 1
    },
    "reporter": {
      "queueSize": 100,
      "bufferFlushInterval": 10,
      "logSpans": false,
      "localAgentHostPort": "jaeger-agent.istio-system:6831"
    },
    "headers": {
      "jaegerDebugHeader": "debug-id",
      "jaegerBaggageHeader": "baggage",
      "TraceContextHeaderName": "uber-trace-id",
      "traceBaggageHeaderPrefix": "testctx-"
    },
    "baggage_restrictions": {
        "denyBaggageOnInitializationFailure": false,
        "hostPort": "jaeger-agent.istio-system:5778",
        "refreshInterval": 60
    }
}
EOF
-----

.. Pay special attention to the value of _localAgentHostPort_ .
+
Thi is the URL that your API gateway will push traces (via UDP) to the _jaeger-agent_ service host and port.


. Create a configmap from the opentracing json file:
+
-----
$ oc create configmap jaeger-config --from-file=$HOME/lab/jaeger_config.json -n $MSA_PROJECT
-----

. Mount the configmap to your opentracing enabled apicast:
+
-----
$ oc volume deploy/$OCP_USERNAME-prod-apicast-istio --add -m /tmp/jaeger/ --configmap-name jaeger-config -n $MSA_PROJECT
-----

. Set environment variables that indicate to the API gateway where to read opentracing related configurations:
+
-----
$ oc env deploy/$OCP_USERNAME-prod-apicast-istio \
         OPENTRACING_TRACER=jaeger \
         OPENTRACING_CONFIG=/tmp/jaeger/jaeger_config.json \
         -n $MSA_PROJECT
-----

. Update the API gateway _deployment_ to use the Opentracing and Jaeger enabled image:
+
-----
$ oc patch deploy/$OCP_USERNAME-prod-apicast-istio \
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"'$OCP_USERNAME'-prod-apicast-istio", "image": "quay.io/3scale/apicast:master" }]}}}}'
-----
+
Notice the use of a community version of the API gateway container image available in a public _quay.io_ organization.
This is the container image that includes the _opentracing_ and _jaeger_ client libraries.

. Resume your Istio and opentracing enabled API gateway.
+
-----
$ oc rollout resume deploy $OCP_USERNAME-prod-apicast-istio
-----

. Verify the existence of the opentracing library for NGinx in the API gateway.
+
Once your API gateway is back up and running, execute the following command :
+
-----
$ oc rsh `oc get pod | grep "apicast-istio" | awk '{print $1}'` ls -l /usr/local/openresty/nginx/modules/ngx_http_opentracing_module.so

-rwxr-xr-x. 1 root root 1457848 Jun 11 06:29 /usr/local/openresty/nginx/modules/ngx_http_opentracing_module.so
-----

. Verify the existence of the jaeger client library in the API gateway:
+
-----
$ oc rsh `oc get pod | grep "apicast-istio" | awk '{print $1}'` ls -l /opt/app-root/lib/libjaegertracing.so.0

lrwxrwxrwx. 1 root root 25 Jun 11 06:38 /opt/app-root/lib/libjaegertracing.so.0 -> libjaegertracing.so.0.3.0
-----


== Jaeger UI


Now that you are using the Opentracing enabled apicast, let's familiarize ourselves the Jaeger user interface to visualize traces that originate from it.

=== Overview

Often the first thing to understand about your microservices architecture is specifically which microservices are involved in an end-user transaction.

Istio supports both Zipkin and Jaeger.
For the purpose of this lab, the focus is on Jaeger.

One important term to understand is _span_.
Jaeger defines span as: “a logical unit of work in the system that has an operation name, the start time of the operation, and the duration. Spans can be nested and ordered to model causal relationships.
An RPC call is an example of a span.”

Another important term to understand is _trace_

Jaeger defines _trace_ as “adata/execution path through the system, and can be thought of as a directed acyclic graph of spans"

=== Procedure

. Identify the URL to the Jaeger UI:
+
-----
$ echo -en "\n\nhttp://"$(oc get route/tracing -o template --template {{.spec.host}} -n istio-system)"\n\n"
-----
+
Using your browser, navigate to this URL.

. In the _Find Traces_ panel, scroll down to locate the traces associated with your OCP user name:
+
image::images/trace_dropdown_selection.png[]

. Click `Find Traces`.
+
You should see an overview with timeline of all of your traces:
+
image::images/trace_overview.png[]

Traces pertaining to your Istio enabled API gateway are now available .
However, what is missing is tracing that includes the backend _catalog_ service.

In the next section, you'll enable your _catalog_ service to provide tracing data .

== Catalog Service: OpenTracing and Istio enabled

image::images/deployment_catalog-istio.png[]

In the above diagram, notice the introduction of a new pod: _catalog-service-istio_.

Ingress requests through the _catalog-service_ are now directed to this new Istio enabled _catalog_ pod (instead of the original _catalog_ pod that is not Istio enabled).

The new catalog service is enabled with _opentracing_ and _jaeger_ libraries so that it can also participate in distributed tracing .

=== OpenTracing libraries included in catalog_service

The _catalog service_ is link:https://github.com/gpe-mw-training/appmod_vertx_experienced/tree/rhte_opentracing/lab-04[written in Java] (specifically using the _reactive_ programming framework link:https://vertx.io/[vert.x]).

As such, the new catalog service used in the remainder of this course is embedded with the opentracing and jaeger Java client libraries.

Recall that when configuring the NGinx and C++ opentracing/jaeger client libraries in the API gateway, a configuration file (via a config map) was loaded.
The opentracing and jaeger client libraries for Java are a bit easier to work with.
The Java client libraries allow for configuration via environment variables.

The Dockerfile with environment variables (and their default values) utilized in building the new opentracing catalog image is as follows:

-----
FROM fabric8/java-jboss-openjdk8-jdk:1.3.1
ENV JAVA_APP_DIR=/deployments
ENV AB_OFF=true
ENV JAEGER_SERVICE_NAME=catalog\
  JAEGER_PROPAGATION=b3\
  JAEGER_SAMPLER_TYPE=const\
  JAEGER_SAMPLER_PARAM=1 \
  JAEGER_AGENT_HOST=jaeger-agent \
  JAEGER_AGENT_PORT=6831
EXPOSE 8080 8778 9779
COPY target/catalog-service-tracing-1.0.7.jar /deployments/
-----

These environment variables can be over-written at deployment time.
For the purpose of this lab, the _JAEGER_AGENT_HOST_ will need to be over-written so that the Java Jaeger client libraries pushes traces to the service endpoint at:  `jaeger-agent.istio-system.svc.cluster.local`

=== Inject Envoy into _catalog_ service

. Retrieve yaml representation of current _catalog service_ deployment:
+
-----
$ oc get deploy catalog-service -n $MSA_PROJECT -o yaml > $HOME/lab/catalog-service.yml
-----

. Differentiate your Istio enabled catalog service from your existing catalog service:
+
-----
$ sed -i "s/ catalog-service/ $OCP_USERNAME-cat-service-istio/" $HOME/lab/catalog-service.yml
-----

. Place the deployment in a paused state:
+
-----
$ sed -i "s/replicas:\ 1/replicas: 1\n  paused: true/" $HOME/lab/catalog-service.yml
-----


. Inject Istio configs into a new catalog service deployment
+
-----

$ istioctl kube-inject \
           -f $HOME/lab/catalog-service.yml \
           > $HOME/lab/catalog-service-istio.yml
-----

. View Istio injected catalog service deployment descriptor:
+
-----
$ cat $HOME/lab/catalog-service-istio.yml | more
-----

. Deploy a new Istio enabled API gateway production gateway that correctly points to the Jaeger agent in your _istio-system_ namespace:
+
-----
$ oc create \
     -f $HOME/lab/catalog-service-istio.yml \
     -n $MSA_PROJECT



$ oc set env deploy/$OCP_USERNAME-cat-service-istio JAEGER_AGENT_HOST=jaeger-agent.istio-system.svc.cluster.local
$ oc set env deploy/$OCP_USERNAME-cat-service-istio JAEGER_SERVICE_NAME=$OCP_USERNAME-cat-service-istio
-----

. Inject required resource limits and requests into Istio related containers :
+
There is a clusterquota assigned to your OCP user.
This clusterquota requires that all containers (including the _istio-proxy_ and _istio-init_ ) specify _limits_ and _requests_.
+
-----
$ oc patch deploy/$OCP_USERNAME-cat-service-istio \
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"istio-proxy", "resources": {   "limits":{"cpu": "500m","memory": "128Mi"},"requests":{"cpu":"50m","memory":"32Mi"}   }}]}}}}'

$ oc patch deploy/$OCP_USERNAME-cat-service-istio \
   --patch '{"spec":{"template":{"spec":{"initContainers":[{"name":"istio-init", "resources": {   "limits":{"cpu": "500m","memory": "128Mi"},"requests":{"cpu":"50m","memory":"32Mi"}   }}]}}}}'
-----

. Update the new catalog service deployment to use the Opentracing and Jaeger enabled image:
+
-----
$ oc patch deploy/$OCP_USERNAME-cat-service-istio \
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"'$OCP_USERNAME'-cat-service-istio", "image": "docker.io/rhtgptetraining/catalog-service-tracing:1.0.9" }]}}}}'
-----

. Resume the paused deployment:
+
-----
$ oc rollout resume deploy/$OCP_USERNAME-cat-service-istio
-----

. Modify the _service_ to route to new Istio enabled _apicast_
+
-----
$ oc patch service/catalog-service \
   --patch '{"spec":{"selector":{"deployment":"'$OCP_USERNAME'-cat-service-istio"}}}'
-----

. Make sure that your `$CATALOG_USER_KEY` environment variable is set:
+
-----
$ echo $CATALOG_USER_KEY

d59904ad4515522ecccb8b81c761a283
-----

. From the terminal, execute the following:
+
-----
$ curl -v -k `echo "https://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----

. Return back to the Jaeger UI and locate your traces.
+
Drill into them and notice the _spans_.

== Istio Ingress gateway

https://istio.io/docs/tasks/traffic-management/ingress/

-----
$ CATALOG_API_GW_HOST=`oc get route/catalog-prod-apicast-$OCP_USERNAME -o template --template {{.spec.host}}`
-----

-----
echo \
    "apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: catalog-istio-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    hosts:
    - "$CATALOG_API_GW_HOST"" \
 > $HOME/lab/catalog-istio-gateway.yml
-----

-----
$ oc create -f $HOME/lab/catalog-istio-gateway.yml --as=system:admin
-----

-----
echo \
    "apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: catalog-istio-gateway-vs
spec:
  hosts:
  - "$CATALOG_API_GW_HOST"
  gateways:
  - catalog-istio-gateway
  http:
  - match:
    - uri:
        prefix: /products
    route:
    - destination:
        port:
          number: 8080
        host: apicast-production.3scale-mt-adm0" \
> $HOME/lab/catalog-istio-gateway-vs.yml
-----

-----
$ oc create -f $HOME/lab/catalog-istio-gateway-vs.yml --as=system:admin
-----


== 3scale Analytics

Return back to your API Manager as the domain admin and navigate to the _Analytics_ tab at the top.

image::images/3scale_analytics.png[]

Notice that the _hits_ metric for your _catalog_service_ API is automatically depicted.
3scale analytics can depict the total count of _hits_ on both the API as well as the API method level graphed over time.

Your API analytics are currently course grained in that the _hits_ are the sum of invocations on all methods of your catalog service.
Defining of fine grained _methods_ and _mappings_ for your catalog API will subsequently provide for more fine grained analytics at the method level.

The analytics provided by 3scale compliment the distributed tracing capabilities of Jaeger.

== 3scale mixer adapter

[red]#TO_DO#

The Red Hat 3scale engineering team is actively working on an Istio _mixer_.
This Istio _mixer_ will allow 3scale API Management policies to be applied directly to the _service mesh_ .


== Conclusions

As you know, Openresty is Nginx + luaJIT, and right now, we only get OpenTracing information for the "Nginx" part of it, there aren't any OpenTracing libraries for lua.
We are working on being able to use the OpenTracing C++ libraries from LUA, so we can create spans directly from it, and gain even more visibility into API gateway internals.
For example, this could help debug if that custom policy you just installed is making things slower.


== Questions



. Which of the following libraries is embedded in community API gateway to support distributed tracing?
+
-----
a) ngx_http_opentracing_module.so
b) libjaegertracing.so.0
c) libzipkintracing.so.0
d) A and B
-----

. What is the name of the CustomResourceDefinition introduced by Istio's v1alpha3 routing API that allows for configuration of an egress route?
+
-----
a) EgressRule
b) DestinationRule
c) ServiceEntry
d) EgressRoute
-----



. The Jaeger java client library provides which of the following features?
+
-----
a) Propogation of traces to the jaeger-agent via UDP on port 6831
b) Propogation of traces to the jaeger-collector via TCP by specifying the environment variable: JAEGER_ENDPOINT
c) Setting of the trace sampler type via the environment variable: JAEGER_SAMPLER_TYPE
d) All of the above
-----

. Which of the following are feature of the Red Hat 3scale product that are not found in Istio ?
+
-----
a) Developer portal
b) Rate limiting
c) Billing
d) A and C
-----

ifdef::showscript[]
1)  answer D
2)  answer C
3)  answer D
4)  answer D
endif::showscript[]

== Appendix


[[configuretestapi]]
=== Configure and Test Basic API Mgmt

In this section, you define a service that manages access to the Coolstore Catalog service that has already been provisioned for you.

The activities in this section are also found in the pre-req courses but is additionally provided here as a refresher for your conveniance.

==== Define Catalog Service

. From the API Manager Admin Portal home page, navigate to the *API* tab.
. On the far right, click image:images/create_service_icon.png[].
. Enter `catalog_service` for the *Name* and *System Name*.
. Select *NGINX API gateway self-managed* *Gateway* type and not a plugin:
+
image::images/apicast_gw.png[]

. Scroll down the page and for the *Authentication* type, select *API Key (user_key)*:
+
image::images/select_api_key.png[]

. Click *Create Service*.

==== Create Application Plan

Application plans define access policies for your API.

. From the *Overview* page of your new `catalog_service`, scroll to the *Published Application Plans* section.
. Click image:images/create_app_plan_icon.png[]:
+
image::images/create_app_plan.png[]

. Enter `catalog_app_plan` for the *Name* and *System name*:

. Click *Create Application Plan*.

==== Create Application

In this section, you associate an application to an API consumer account.
This generates a _user key_ to the application based on the details previously defined in the application plan.
The user key is used as a query parameter to the HTTP request to invoke your business services via your on-premise API gateway.

. Navigate to the *Developers* tab.
. Select the `Developer` account.
. Create Application
.. Click the *0 Applications* link at the top:
+
NOTE: A default application may have already been created (in which case the link will indicate 1 Application, not 0).
If so, this default application is typically associated with the out-of-the-box `API` service (which is not what you want).
If it exists, feel free to click on default application to identify which service it is associated with and then delete it.

.. Click image:images/create_app_icon.png[].
.. Fill in the *New Application* form as follows:
... *Application plan*: `catalog_app_plan`
... *Service Plan*: `Default`
... *Name*: `catalog_app`
... *Description*: `catalog_app`
+
image::images/create_catalog_app.png[]

.. Click *Create Application*.

. On the details page for your new application (or the default application automatically created), find the API *User Key*:
+
image::images/new_catalog_user_key.png[]

. Create an environment variable set to this user key:
+
-----
$ export CATALOG_USER_KEY=<the catalog app user key>
-----

==== Stage Service Integration

In this section, you define an _API proxy_ to manage your _catalog_ RESTful business service.

. In the API Manager Admin Portal, navigate to the *APIs* tab.
. From your `catalog_service` service, select *Application Plans*.
. For the  `catalog_app_plan` and click the *Publish* link:
+
image::images/publish_app_plan.png[]
. From your `catalog_service` service, select *Integration*.
. Click *Add the base URL of your API and save the configuration*.
+
* This takes you to a page that allows you to associate the API gateway staging and production URLs with your new 3scale proxy service.

. Populate the *Configuration: configure & test immediately in the staging environment* form as follows:
.. *Private Base URL*:
... Enter the internal DNS resolvable URL to your Catalog business service.
... The internal URL will be the output of the following:
+
-----
$ echo -en "\n\nhttp://catalog-service.$MSA_PROJECT.svc.cluster.local:8080\n\n"
-----

.. *Staging Public Base URL*: Populate this field with the output from the following command:
+
-----
$ echo -en "\n`oc get route catalog-stage-apicast-$OCP_USERNAME -n $MSA_PROJECT --template "https://{{.spec.host}}"`:443\n\n"
-----

.. *Production Public Base URL*: Populate this field with the output from the following command:
+
-----
$ echo -en "\n`oc get route catalog-prod-apicast-$OCP_USERNAME -n $MSA_PROJECT --template "https://{{.spec.host}}"`:443\n\n"
-----

.. *API test GET request*: Enter `/products`.

** Expect to see a test cURL command populated with the API key assigned to you for the `catalog_app_plan`:
+
image::images/apikey_shows_up.png[]
+
.. If not, go back through the steps to create an Application Plan and corresponding Application.
+
NOTE: When there are multiple developer accounts, Red Hat 3scale API Management uses the default developer account that is created with every new API provider account to determine which user key to use. When creating new services, the API Manager sets the first application from the first account subscribed to the new service as the default.

. Click *Update & test in Staging Environment*
.. In doing so, the `apicast-stage` pod invokes your backend _catalog_ business service as per the `Private Base URL`.
.. The page should turn green with a message indicating success.
+
image::images/stage_success.png[]

. Click *Back to Integration & Configuration*:
. Click *Promote v. 1 to Production*:
+
image::images/stage_and_prod.png[]

Your 3scale by Red Hat service is configured.
Next, the configuration details of your service need to be propagated to your on-premise API gateway.

==== Refresh API gateway at boot
Every time a configuration change is made to an api proxy or application plan, the production API gateways need to be refreshed with the latest changes.

The API gateways are configured to refresh the latest configuration information from the API management platform every 5 minutes.
When this internal NGINX timer is triggered, you see log statements in your API gateway similar to the following:

.Sample Output
-----
[debug] 36#36: *3574 [lua] configuration_loader.lua:132: updated configuration via timer:

....

[info] 36#36: *3574 [lua] configuration_loader.lua:160: auto updating configuration finished successfuly, context: ngx.timer
-----

For the purpose of this lab, instead of potentially waiting for 5 minutes, you can simply bounce your API gateway pods .

. Bounce API gateway related pods:
+
-----
$ for i in `oc get pod -n $MSA_PROJECT | grep "apicast" | awk '{print $1}'`; do oc delete pod $i; done
-----
+
Kubernetes will detect the absence of these pods and start new ones.
+
Because the value of the _APICAST_CONFIGURATION_LOADER_ environment variable in the pod is set to `boot`, the service proxy configuration from the API Manager will automatically be pulled upon restart.

. Tail the log of the new API gateway production pod.

* A debug-level log statement similar to the following appears:
+
.Sample Output
-----
[lua] configuration_store.lua:103: configure(): added service 2555417742084 configuration with hosts: prod-apicast-user1.apps.7777.thinkpadratwater.com, catalog-stage-apicast-user1.apps.7777.thinkpadratwater.com ttl: 300
-----

==== Test Catalog Business Service

In this section, you invoke your Catalog business service via your production API gateway.

. Make sure that your `$CATALOG_USER_KEY` environment variable is still set:
+
-----
$ echo $CATALOG_USER_KEY
-----

. From the terminal, execute the following:
+
-----
$ curl -v -k `echo "https://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----
+
.Sample Output
-----
...

{
  "itemId" : "444435",
  "name" : "Oculus Rift",
  "desc" : "The world of gaming has also undergone some very unique and compelling tech advances in recent years. Virtual reality, the concept of complete immersion into a digital universe through a special headset, has been the white whale of gaming and digital technology ever since Nintendo marketed its Virtual Boy gaming system in 1995.",
  "price" : 106.0
}
-----

. If you are still tailing the log of your `apicast` pod, expect to see statements similar to this:
+
.Sample Output
-----
...

2018/08/06 19:07:46 [info] 24#24: *19 [lua] backend_client.lua:108: authrep(): backend client uri: http://backend-listener.3scale-mt-adm0:3000/transactions/authrep.xml?service_token=a4e0949f1b677611870dab3fb7c142df50871d1eca3d1c9f1615dd514c937df4&service_id=103&usage%5Bhits%5D=1&user_key=ccc4cbae7a44b363a6cd5907a54ff2f9 ok: true status: 200 body:  while sending to client, client: 172.17.0.1, server: _, request: "GET /products?user_key=ccc4cbae7a44b363a6cd5907a54ff2f9 HTTP/1.1", host: "catalog-service.rhte-mw-api-mesh-user1.svc.cluster.local"

...

-----




ifdef::showscript[]

echo -en "\n\ncurl -k ${THREESCALE_PORTAL_ENDPOINT}/admin/api/services.json\n\n"                                    :   test retrival of proxy service info from system-provider

oc rsh `oc get pod | grep "prod-apicast-istio" | awk '{print $1}'` curl localhost:8090/status/live                  :   test liveness probe of istio enabled apicast
oc rsh `oc get pod | grep "prod-apicast-istio" | awk '{print $1}'` curl localhost:8090/status/ready                 :   test readiness probe of istio enabled apicast

oc rsh `oc get pod | grep "apicast-istio" | awk '{print $1}'`                                                       :   ssh into istio enabled API gateway gw

oc logs -f  `oc get pod | grep "apicast-istio" \
            | grep "Running" \
            | awk '{print $1}'` -c $OCP_USERNAME-prod-apicast-istio                                                 :   log of istio enabled API gateway gw

for i in `oc get pod | grep "apicast-istio" | awk '{print $1}'`; do oc delete pod $i; done                          :   Re-dploy Istio enabled API gateway


TO-DO
  1)  Is a liveness probe necessary for API gateway ?  API gateway appears to error out on its own during boot problems.
  2)  With liveness and readiness probes removed, API gateway boot error behaves differently depending on whether it is injected with istio
        - istio injected :   API gateway boot errors cause fail-over the first 2 or 3 times.  Then no longer any errors.
        - no istio       :   API gateway continues to fail upon boot errors

      Turns out envoy proxy is blocking outbound calls at boot for about 1 minute or so
      All outbound calls from primary pods (ie:  API gateway invocation to THREESCALE_PORTAL_ENDPOINT and vert.x / fabric8 invocation to kubernetes API to query for configmap) during that time are blocked.

      https://github.com/istio/istio/issues/3533        :   startup time of istio-proxy causes comm issues for up to 30 seconds


  3) investigate istio-ingress
        OCP ha-proxy -> istio-ingress -> API gateway gw -> catalog service

  4) when API gateway is in info log level, why does it stop rebooting itself when a THREESCALE_PORTAL_ENDPOINT related problem is encountered ?
     when API gateway is in debug log level, it continues to cycle when it encounters a THREESCALE_PORTAL_ENDPOINT problem .

  5) with istio injected apicast, boot doesn't start however a curl within the same pod on THREESCALE_PORTAL_ENDPOINT does work

  6) allow user write access to istio-system to allow for execution of:  "istioctl create"

  7) opentracing enabled apicast
        - quay.io/3scale/apicast:master
        - OPENTRACING_TRACER:           Which Tracer implementation to use, right now, only Jaeger is available.
        - OPENTRACING_CONFIG:           Each tracer has a default configuration file, you can see an example here: jaeger.example.json
        - OPENTRACING_HEADER_FORWARD:   By default, uses uber-trace-id, if your OpenTracing has a different configuration, you will need to change this value, if not, ignore it.



        .. [red]#TODO Investigate why liveness and readiness probes are failing#
        +
        -----
        Readiness probe failed: Get http://10.1.3.121:8090/status/ready: dial tcp 10.1.3.121:8090: getsockopt: connection refused
        Liveness probe failed: Get http://10.1.3.121:8090/status/live: dial tcp 10.1.3.121:8090: getsockopt: connection refused

        -----



The _info_ log level in API gateway actually provides more useful connection error details than does the _debug_ log level.
+
This will become important because we are about to encounter a connection related error now that Istio is introduced .
The connection problem will be in the API gateway at boot when it attempts to pull (using the value set in its THREESCALE_PORTAL_ENDPOINT env variable) _proxy-config_ information from the _system-provider_ of the API Manager.

. Investigate _apicast_ provisioning problem
+
-----
$ oc logs -f `oc get pod | grep "apicast-istio" | awk '{print $1}'` -c $OCP_USERNAME-prod-apicast-istio

...

2018/08/02 08:32:23 [warn] 23#23: *2 [lua] remote_v2.lua:163: call(): failed to get list of services: invalid status: 0 url: https://user1-3scale-admin.apps.7777.thinkpadratwater.com/admin/api/services.json, context: ngx.timer
2018/08/02 08:32:23 [info] 23#23: *2 [lua] remote_v1.lua:98: call(): configuration request sent: https://user1-3scale-admin.apps.7777.thinkpadratwater.com/admin/api/nginx/spec.json, context: ngx.timer
2018/08/02 08:32:23 [error] 23#23: *2 peer closed connection in SSL handshake, context: ngx.timer
2018/08/02 08:32:23 [warn] 23#23: *2 [lua] remote_v1.lua:108: call(): configuration download error: handshake failed, context: ngx.timer
ERROR: /opt/app-root/src/src/apicast/configuration_loader.lua:57: missing configuration
stack traceback:
	/opt/app-root/src/src/apicast/configuration_loader.lua:57: in function 'boot'
	/opt/app-root/src/libexec/boot.lua:6: in function 'file_gen'
	init_worker_by_lua:49: in function <init_worker_by_lua:47>
	[C]: in function 'xpcall'
	init_worker_by_lua:56: in function <init_worker_by_lua:54>

-----

.. From the log file, notice that initial warning indicates a failure "to get list services" from the API Manager _system-provider_ service.
+
Why would you expect that the _curl_ utility to be able to pull the _service-proxy_ data when rsh'd into the API gateway but the API gateway itself fails to do so ?



== istio / OCP workshop problem

[2018-08-11 21:02:53.607][154][info][config] external/envoy/source/server/listener_manager_impl.cc:903] all dependencies initialized. starting workers
2018-08-11T21:02:57.106685Z	warn	Epoch 0 terminated with an error: signal: killed
2018-08-11T21:02:57.106713Z	warn	Aborted all epochs
2018-08-11T21:02:57.106739Z	info	Epoch 0: set retry delay to 3.2s, budget to 5
2018-08-11T21:03:00.306904Z	info	Reconciling configuration (budget 5)


=== Lab Focus: Configuration

The emphasis of this lab is on configuration: specifically, configuration of a _Cloud Native _ application managed by 3scale and an Istio  _Service Mesh_.

Students of this lab will not write any business logic.

Development of cloud native applications can be written in a wide variety of development platforms offered by Red Hat to include:

. Red Hat Openshift Application Runtimes (RHOAR)
. Red Hat Fuse on OpenShift

Details about these Red Hat development platforms are out of scope for this specific lab.


endif::showscript[]
